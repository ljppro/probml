\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{Preface}{xxxiii}{chapter*.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Figures}{xxxv}{chapter*.7}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{I\hspace {1em}Fundamentals}{3}{part.1}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}Probability}{5}{chapter.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}Introduction}{5}{section.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}Some common probability distributions}{5}{section.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}Discrete distributions}{5}{subsection.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.1}Bernoulli and binomial distributions}{5}{subsubsection.2.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.2}Categorical and multinomial distributions}{6}{subsubsection.2.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.3}Poisson distribution}{6}{subsubsection.2.2.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}Continuous distributions on $\mathbb {R}$}{6}{subsection.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.1}Gaussian (Normal)}{6}{subsubsection.2.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.2}Half-normal}{7}{subsubsection.2.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.3}Student $t$ distribution}{7}{subsubsection.2.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.4}Cauchy distribution}{8}{subsubsection.2.2.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.5}Laplace distribution}{8}{subsubsection.2.2.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.2.6}Sub-Gaussian and super-Gaussian distributions}{9}{subsubsection.2.2.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}Continuous distributions on $\mathbb {R}^+$}{9}{subsection.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.1}Gamma distribution}{10}{subsubsection.2.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.2}Exponential distribution}{10}{subsubsection.2.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.3}Chi-squared distribution}{10}{subsubsection.2.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.4}Inverse gamma}{10}{subsubsection.2.2.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.5}Pareto distribution}{11}{subsubsection.2.2.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.4}Continuous distributions on $[0,1]$}{12}{subsection.2.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.4.1}Beta distribution}{12}{subsubsection.2.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.5}The multivariate Gaussian (normal) distribution}{13}{subsection.2.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.5.1}Marginals and conditionals of an MVN}{13}{subsubsection.2.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.5.2}Bayes' rule for linear Gaussian systems}{14}{subsubsection.2.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.6}Other multivariate distributions}{15}{subsection.2.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.6.1}Multivariate Student distribution}{15}{subsubsection.2.2.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.6.2}Wishart distribution}{15}{subsubsection.2.2.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.6.3}Dirichlet distribution}{15}{subsubsection.2.2.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}The exponential family}{16}{section.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}Definition}{17}{subsection.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.2}Examples}{18}{subsection.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.1}Bernoulli distribution}{18}{subsubsection.2.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.2}Categorical distribution}{19}{subsubsection.2.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.3}Univariate Gaussian}{20}{subsubsection.2.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.4}Univariate Gaussian with fixed variance}{20}{subsubsection.2.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.5}Multivariate Gaussian}{21}{subsubsection.2.3.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.2.6}Non-examples}{22}{subsubsection.2.3.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.3}Log partition function is cumulant generating function}{22}{subsection.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.1}Derivation of the mean}{23}{subsubsection.2.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.2}Derivation of the variance}{23}{subsubsection.2.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.3}Connection with the Fisher information matrix}{24}{subsubsection.2.3.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.4}Canonical (natural) vs mean (moment) parameters}{24}{subsection.2.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.5}MLE for the exponential family}{25}{subsection.2.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.6}Exponential dispersion family}{26}{subsection.2.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.7}Maximum entropy derivation of the exponential family}{26}{subsection.2.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}Fisher information matrix (FIM)}{27}{section.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}Definition}{27}{subsection.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.2}Equivalence between the FIM and the Hessian of the NLL}{27}{subsection.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.3}Examples}{29}{subsection.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.3.1}FIM for the Binomial}{29}{subsubsection.2.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.3.2}FIM for the Gaussian}{29}{subsubsection.2.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.3.3}FIM for logistic regression}{29}{subsubsection.2.4.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.4}Approximating KL divergence using FIM}{30}{subsection.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.5}Fisher information matrix for exponential family}{30}{subsection.2.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}Transformations of random variables}{31}{section.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.1}Invertible transformations (bijections)}{32}{subsection.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.2}Monte Carlo approximation}{32}{subsection.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.5.3}Probability integral transform}{33}{subsection.2.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.6}Markov chains}{33}{section.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.1}Parameterization}{34}{subsection.2.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.1.1}Markov transition kernels}{34}{subsubsection.2.6.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.1.2}Markov transition matrices}{35}{subsubsection.2.6.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.1.3}Higher-order Markov models}{36}{subsubsection.2.6.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.2}Application: Language modeling}{37}{subsection.2.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.3}Parameter estimation}{37}{subsection.2.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.3.1}Maximum likelihood estimation}{38}{subsubsection.2.6.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.3.2}Sparse data problem}{38}{subsubsection.2.6.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.3.3}MAP estimation}{39}{subsubsection.2.6.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.6.4}Stationary distribution of a Markov chain}{39}{subsection.2.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.4.1}What is a stationary distribution?}{39}{subsubsection.2.6.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.4.2}Computing the stationary distribution}{40}{subsubsection.2.6.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.4.3}When does a stationary distribution exist?}{41}{subsubsection.2.6.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.6.4.4}Detailed balance}{42}{subsubsection.2.6.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.7}Divergence measures between probability distributions}{43}{section.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.1}f-divergence}{43}{subsection.2.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.1}KL divergence}{44}{subsubsection.2.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.2}Alpha divergence}{44}{subsubsection.2.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.3}Hellinger distance}{44}{subsubsection.2.7.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.1.4}Chi-squared distance}{45}{subsubsection.2.7.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.2}Integral probability metrics}{45}{subsection.2.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.3}Maximum mean discrepancy (MMD)}{46}{subsection.2.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.1}MMD as an IPM}{46}{subsubsection.2.7.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.2}Computing the MMD using the kernel trick}{47}{subsubsection.2.7.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.3}Linear time computation}{47}{subsubsection.2.7.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.3.4}Choosing the right kernel}{47}{subsubsection.2.7.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.4}Total variation distance}{48}{subsection.2.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.5}Comparing distributions using binary classifiers}{48}{subsection.2.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}Bayesian statistics}{51}{chapter.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}Introduction}{51}{section.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.1}Frequentist statistics}{51}{subsection.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.2}Bayesian statistics}{51}{subsection.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.3}Arguments for the Bayesian approach}{52}{subsection.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.1}De Finetti's theorem}{52}{subsubsection.3.1.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.2}The Dutch book theorem}{53}{subsubsection.3.1.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.3}Online learning}{53}{subsubsection.3.1.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.4}Arguments against the Bayesian approach}{53}{subsection.3.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.5}Why not just use MAP estimation?}{53}{subsection.3.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.1}The MAP estimate gives no measure of uncertainty}{54}{subsubsection.3.1.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.2}The plugin approximation does not capture predictive uncertainty}{54}{subsubsection.3.1.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.3}The MAP estimate is often untypical of the posterior}{56}{subsubsection.3.1.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.4}The MAP estimate is only optimal for 0-1 loss}{57}{subsubsection.3.1.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.5}The MAP estimate is not invariant to reparameterization}{57}{subsubsection.3.1.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.5.6}MAP estimation cannot handle the cold-start problem}{57}{subsubsection.3.1.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}Closed-form analysis using conjugate priors}{58}{section.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.1}The binomial model}{58}{subsection.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.2}The multinomial model}{59}{subsection.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.3}The univariate Gaussian model}{60}{subsection.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.1}Posterior of $\mu $ given $\sigma ^2$}{60}{subsubsection.3.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.2}Posterior of $\sigma ^2$ given $\mu $}{62}{subsubsection.3.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.3}Posterior of $\mu $ and $\sigma ^2$: conjugate prior}{63}{subsubsection.3.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.3.4}Posterior of $\mu $ and $\sigma ^2$: uninformative prior}{64}{subsubsection.3.2.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.4}The multivariate Gaussian model}{65}{subsection.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.5}Conjugate-exponential models}{65}{subsection.3.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.5.1}Likelihood}{66}{subsubsection.3.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.5.2}Prior}{66}{subsubsection.3.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.5.3}Posterior}{66}{subsubsection.3.2.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.5.4}Posterior predictive density}{67}{subsubsection.3.2.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.2.5.5}Example: Bernoulli distribution}{67}{subsubsection.3.2.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}Beyond conjugate priors}{68}{section.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.3.1}Robust (heavy-tailed) priors}{68}{subsection.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.3.2}Priors for variance parameters}{69}{subsection.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.2.1}Prior for variance terms}{69}{subsubsection.3.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.3.2.2}Priors for covariance matrices}{69}{subsubsection.3.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.4}Noninformative priors}{70}{section.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.1}Maximum entropy priors}{71}{subsection.3.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.2}Jeffreys priors}{71}{subsection.3.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.2.1}Jeffreys prior for binomial distribution}{72}{subsubsection.3.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.2.2}Jeffreys prior for multinomial distribution}{74}{subsubsection.3.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.2.3}Jeffreys prior for the mean and variance of a univariate Gaussian}{74}{subsubsection.3.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.3}Invariant priors}{74}{subsection.3.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.1}Translation-invariant priors}{74}{subsubsection.3.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.2}Scale-invariant prior}{75}{subsubsection.3.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.4.3.3}Learning invariant priors}{75}{subsubsection.3.4.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.4.4}Reference priors}{75}{subsection.3.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.5}Hierarchical priors}{76}{section.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.1}A hierarchical binomial model}{77}{subsection.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.5.2}A hierarchical Gaussian model}{78}{subsection.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.2.1}Example: the 8-schools dataset}{79}{subsubsection.3.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.5.2.2}Non-centered parameterization}{80}{subsubsection.3.5.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.6}Empirical Bayes}{82}{section.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.6.1}A hierarchical binomial model}{82}{subsection.3.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.6.2}A hierarchical Gaussian model}{83}{subsection.3.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.6.3}Hierarchical Bayes for n-gram smoothing}{84}{subsection.3.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.7}Model selection and evaluation}{86}{section.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.1}Bayesian model selection}{86}{subsection.3.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.2}Estimating the marginal likelihood}{87}{subsection.3.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.3}Connection between cross validation and marginal likelihood}{88}{subsection.3.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.4}Pareto-Smoothed Importance Sampling LOO estimate}{89}{subsection.3.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.5}Information criteria}{90}{subsection.3.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.5.1}Minimum description length (MDL)}{91}{subsubsection.3.7.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.5.2}The Bayesian information criterion (BIC)}{91}{subsubsection.3.7.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.5.3}Akaike information criterion}{92}{subsubsection.3.7.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.5.4}Widely applicable information criterion (WAIC)}{92}{subsubsection.3.7.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.6}Posterior predictive checks}{93}{subsection.3.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.6.1}Example: 1d Gaussian}{93}{subsubsection.3.7.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.7.7}Bayesian p-values}{94}{subsection.3.7.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.7.7.1}Example: linear regression}{95}{subsubsection.3.7.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.8}Bayesian decision theory}{96}{section.3.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.1}Basics}{96}{subsection.3.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.2}Example: classification}{96}{subsection.3.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.3}Regression}{97}{subsection.3.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.4}Structured prediction}{97}{subsection.3.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.5}COVID-19}{99}{subsection.3.8.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.8.6}Multi-stage decision problems}{100}{subsection.3.8.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}Probabilistic graphical models}{101}{chapter.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}Introduction}{101}{section.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}Directed graphical models (Bayes nets)}{101}{section.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.1}Representing the joint distribution}{101}{subsection.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.2}Examples}{102}{subsection.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.2.1}Markov chains}{102}{subsubsection.4.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.2.2}The ``Student'' network}{103}{subsubsection.4.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.2.3}Sigmoid belief nets}{105}{subsubsection.4.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.3}Gaussian Bayes nets}{106}{subsection.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.4}Conditional independence properties}{107}{subsection.4.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.4.1}Global Markov properties (d-separation)}{107}{subsubsection.4.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.4.2}Explaining away (Berkson's paradox)}{109}{subsubsection.4.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.4.3}Other Markov properties}{111}{subsubsection.4.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.4.4}Markov blankets and full conditionals}{111}{subsubsection.4.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.4.5}I-maps}{112}{subsubsection.4.2.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.5}Generation (sampling)}{112}{subsection.4.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.6}Inference}{112}{subsection.4.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.6.1}Example: inference in the Student network}{113}{subsubsection.4.2.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.7}Learning}{114}{subsection.4.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.1}Learning from complete data}{114}{subsubsection.4.2.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.2}Example: computing the MLE for CPTs}{115}{subsubsection.4.2.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.3}Example: Computing the posterior for CPTs}{116}{subsubsection.4.2.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.4}Learning from incomplete data}{117}{subsubsection.4.2.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.5}Using EM to fit CPTs in the incomplete data case}{117}{subsubsection.4.2.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.7.6}Using SGD to fit CPTs in the incomplete data case}{118}{subsubsection.4.2.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.8}Plate notation}{119}{subsection.4.2.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.8.1}Example: factor analysis}{120}{subsubsection.4.2.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.8.2}Example: Naive Bayes classifier}{121}{subsubsection.4.2.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.2.8.3}Example: relaxing the naive Bayes assumption}{121}{subsubsection.4.2.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}Undirected graphical models (Markov random fields)}{122}{section.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.1}Representing the joint distribution}{122}{subsection.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.1.1}Hammersley-Clifford theorem}{123}{subsubsection.4.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.1.2}Gibbs distribution}{124}{subsubsection.4.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.2}Fully visible MRFs (Ising, Potts, Hopfield, etc)}{124}{subsection.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.2.1}Ising models}{124}{subsubsection.4.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.2.2}Potts models}{126}{subsubsection.4.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.2.3}Potts models for protein structure prediction}{127}{subsubsection.4.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.2.4}Hopfield networks}{128}{subsubsection.4.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.3}MRFs with latent variables (Boltzmann machines, etc)}{130}{subsection.4.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.3.1}Vannilla Boltzmann machines}{130}{subsubsection.4.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.3.2}Restricted Boltzmann machines (RBMs)}{130}{subsubsection.4.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.3.3}Deep Boltzmann machines}{131}{subsubsection.4.3.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.3.4}Deep belief networks (DBNs)}{132}{subsubsection.4.3.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.4}Maximum entropy models}{132}{subsection.4.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.4.1}Log-linear models}{133}{subsubsection.4.3.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.4.2}Feature induction for a maxent spelling model}{133}{subsubsection.4.3.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.5}Gaussian MRFs}{134}{subsection.4.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.5.1}Standard GMRFs}{135}{subsubsection.4.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.5.2}Nonlinear Gaussian MRFs}{136}{subsubsection.4.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.6}Conditional independence properties}{137}{subsection.4.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.6.1}Basic results}{137}{subsubsection.4.3.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.6.2}An undirected alternative to d-separation}{138}{subsubsection.4.3.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.7}Generation (sampling)}{139}{subsection.4.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.8}Inference}{139}{subsection.4.3.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.9}Learning}{140}{subsection.4.3.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.9.1}Learning from complete data}{140}{subsubsection.4.3.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.9.2}Computational issues}{141}{subsubsection.4.3.9.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.9.3}Maximum pseudo-likelihood estimation}{142}{subsubsection.4.3.9.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.3.9.4}Learning from incomplete data}{143}{subsubsection.4.3.9.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.4}Conditional random fields (CRFs)}{144}{section.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.4.1}1d CRFs}{144}{subsection.4.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.1.1}Noun phrase chunking}{145}{subsubsection.4.4.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.1.2}Named entity recognition}{146}{subsubsection.4.4.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.1.3}Natural language parsing}{147}{subsubsection.4.4.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.4.2}2d CRFs}{147}{subsection.4.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.2.1}Semantic segmentation}{148}{subsubsection.4.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.2.2}Deformable parts models}{149}{subsubsection.4.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.4.3}Parameter estimation}{150}{subsection.4.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.3.1}Log-linear potentials}{150}{subsubsection.4.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.4.3.2}General case}{151}{subsubsection.4.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.4.4}Other approaches to structured prediction}{151}{subsection.4.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.5}Comparing directed and undirected PGMs}{151}{section.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.1}CI properties}{151}{subsection.4.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.2}Converting between a directed and undirected model}{153}{subsection.4.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.5.2.1}Converting a DPGM\xspace to a UPGM\xspace }{153}{subsubsection.4.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.5.2.2}Converting a UPGM\xspace to a DPGM\xspace }{154}{subsubsection.4.5.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.3}Conditional directed vs undirected PGMs and the label bias problem}{154}{subsection.4.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.4}Combining directed and undirected graphs}{155}{subsection.4.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.5.4.1}Chain graphs}{155}{subsubsection.4.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.5.4.2}Acyclic directed mixed graphs}{156}{subsubsection.4.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.5.5}Comparing directed and undirected Gaussian PGMs}{157}{subsection.4.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.5.5.1}Covariance graphs}{158}{subsubsection.4.5.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.6}PGM extensions}{159}{section.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.1}Factor graphs}{159}{subsection.4.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.6.1.1}Bipartite factor graphs}{159}{subsubsection.4.6.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.6.1.2}Forney factor graphs}{160}{subsubsection.4.6.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.2}Probabilistic circuits}{162}{subsection.4.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.3}Directed relational PGMs}{162}{subsection.4.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.4}Undirected relational PGMs}{164}{subsection.4.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.6.4.1}Collective classification}{165}{subsubsection.4.6.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.6.4.2}Markov logic networks}{165}{subsubsection.4.6.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.5}Open-universe probability models}{167}{subsection.4.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.6.6}Programs as probability models}{169}{subsection.4.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.7}Structural causal models}{169}{section.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.7.1}Example: causal impact of education on wealth}{170}{subsection.4.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.7.2}Structural equation models}{171}{subsection.4.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.7.3}Do operator and augmented DAGs}{171}{subsection.4.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.7.4}Estimating average treatment effect using path analysis}{173}{subsection.4.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.7.4.1}Direct effect}{173}{subsubsection.4.7.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {4.7.4.2}Indirect effect (mediation analysis)}{173}{subsubsection.4.7.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.7.5}Counterfactuals}{174}{subsection.4.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {5}Information theory}{177}{chapter.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.1}KL divergence}{177}{section.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.1}Desiderata}{178}{subsection.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.2}The KL divergence uniquely satisfies the desiderata}{179}{subsection.5.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.2.1}Continuity of KL}{179}{subsubsection.5.1.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.2.2}Non-negativity of KL divergence}{180}{subsubsection.5.1.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.2.3}KL divergence is invariant to reparameterizations}{180}{subsubsection.5.1.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.2.4}Montonicity for uniform distributions}{181}{subsubsection.5.1.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.2.5}Chain rule for KL divergence}{181}{subsubsection.5.1.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.3}Thinking about KL}{182}{subsection.5.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.3.1}Units of KL}{182}{subsubsection.5.1.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.3.2}Asymmetry of the KL divergence}{182}{subsubsection.5.1.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.3.3}KL as expected weight of evidence}{183}{subsubsection.5.1.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.4}Properties of KL}{184}{subsection.5.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.4.1}Compression Lemma}{184}{subsubsection.5.1.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.4.2}Data processing inequality for KL}{185}{subsubsection.5.1.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.5}KL divergence and MLE}{186}{subsection.5.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.6}KL divergence and Bayesian Inference}{187}{subsection.5.1.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.7}KL divergence and Exponential Families}{188}{subsection.5.1.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.7.1}Example: KL divergence between two Gaussians}{188}{subsubsection.5.1.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.1.8}Bregman divergence}{189}{subsection.5.1.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.1.8.1}KL is a Bregman divergence}{190}{subsubsection.5.1.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.2}Entropy}{190}{section.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.2.1}Definition}{190}{subsection.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.2.2}Differential entropy for continuous random variables}{191}{subsection.5.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.2.3}Typical sets}{192}{subsection.5.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.2.4}Cross entropy and perplexity}{193}{subsection.5.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.3}Mutual information}{194}{section.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.1}Definition}{194}{subsection.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.2}Interpretation}{194}{subsection.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.3}Data processing inequality}{195}{subsection.5.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.4}Sufficient Statistics}{196}{subsection.5.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.5}Multivariate mutual information}{196}{subsection.5.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.5.1}Total correlation}{196}{subsubsection.5.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.5.2}Interaction information (co-information)}{197}{subsubsection.5.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.5.3}Synergy and redundancy}{198}{subsubsection.5.3.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.5.4}MMI and causality}{198}{subsubsection.5.3.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.5.5}MMI and entropy}{198}{subsubsection.5.3.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.3.6}Variational bounds on mutual information}{199}{subsection.5.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.6.1}Upper bound}{199}{subsubsection.5.3.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.6.2}BA lower bound}{200}{subsubsection.5.3.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.6.3}NWJ lower bound}{200}{subsubsection.5.3.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {5.3.6.4}InfoNCE lower bound}{201}{subsubsection.5.3.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.4}Data compression (source coding)}{201}{section.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.1}Lossless compression}{202}{subsection.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.2}Lossy compression and the rate-distortion tradeoff}{202}{subsection.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.3}Bits back coding}{204}{subsection.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.5}Error-correcting codes (channel coding)}{205}{section.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.6}The information bottleneck}{206}{section.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.1}Vanilla IB}{206}{subsection.5.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.2}Variational IB}{207}{subsection.5.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.6.3}Conditional entropy bottleneck}{209}{subsection.5.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {6}Optimization}{211}{chapter.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.1}Introduction}{211}{section.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.2}Automatic differentiation}{211}{section.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.2.1}Differentiation in functional form}{211}{subsection.6.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Linear and multilinear functions.}{212}{section*.123}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{The derivative operator.}{213}{section*.124}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Higher-order derivatives.}{213}{section*.125}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Multiple inputs.}{214}{section*.126}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Composition and fan-out.}{215}{section*.127}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.2.2}Differentiating chains, circuits, and programs}{216}{subsection.6.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.2.2.1}Chain compositions and the chain rule}{216}{subsubsection.6.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.2.2.2}From chains to circuits}{218}{subsubsection.6.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.2.2.3}From circuits to programs}{220}{subsubsection.6.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.3}Stochastic gradient descent}{221}{section.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.4}Natural gradient descent}{222}{section.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.1}Defining the natural gradient}{222}{subsection.6.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.2}Interpretations of NGD}{223}{subsection.6.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.4.2.1}NGD as a trust region method}{223}{subsubsection.6.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.4.2.2}NGD as a Gauss-Newton method}{224}{subsubsection.6.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.3}Benefits of NGD}{224}{subsection.6.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.4}Approximating the natural gradient}{225}{subsection.6.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.4.5}Natural gradients for the exponential family}{226}{subsection.6.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.4.5.1}Analytic computation for the Gaussian case}{227}{subsubsection.6.4.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.4.5.2}Stochastic approximation for the general case}{227}{subsubsection.6.4.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.4.5.3}Natural gradient of the entropy function}{228}{subsubsection.6.4.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.5}Gradients of stochastic functions}{228}{section.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.1}Minibatch approximation to finite-sum objectives}{229}{subsection.6.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.2}Optimizing parameters of a distribution}{229}{subsection.6.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.3}Score function estimator (likelihood ratio trick)}{230}{subsection.6.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.5.3.1}Control variates}{230}{subsubsection.6.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.5.3.2}Rao-Blackwellisation}{231}{subsubsection.6.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.4}Reparameterization trick}{231}{subsection.6.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.5.4.1}Example}{231}{subsubsection.6.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.5.4.2}Total derivative}{232}{subsubsection.6.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.5}The delta method}{233}{subsection.6.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.6}Gumbel softmax trick}{233}{subsection.6.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.7}Stochastic computation graphs}{234}{subsection.6.5.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.5.8}Straight-through estimator}{234}{subsection.6.5.8}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.6}Bound optimization (MM) algorithms}{235}{section.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.1}The general algorithm}{235}{subsection.6.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.2}Example: logistic regression}{236}{subsection.6.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.3}The EM algorithm}{238}{subsection.6.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.3.1}Lower bound}{238}{subsubsection.6.6.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.3.2}E step}{239}{subsubsection.6.6.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.3.3}M step}{239}{subsubsection.6.6.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.4}Example: EM for an MVN with missing data}{240}{subsection.6.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.4.1}E step}{240}{subsubsection.6.6.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.4.2}M step}{241}{subsubsection.6.6.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.4.3}Initialization}{241}{subsubsection.6.6.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.4.4}Example}{241}{subsubsection.6.6.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.5}Example: robust linear regression using Student-$t$ likelihood}{242}{subsection.6.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.6.6}Extensions to EM}{243}{subsection.6.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.1}Variational EM}{243}{subsubsection.6.6.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.2}Hard EM}{244}{subsubsection.6.6.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.3}Monte Carlo EM}{244}{subsubsection.6.6.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.4}Generalized EM}{244}{subsubsection.6.6.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.5}ECM algorithm}{244}{subsubsection.6.6.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.6.6.6}Online EM}{245}{subsubsection.6.6.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.7}The Bayesian learning rule}{245}{section.6.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.7.1}Deriving inference algorithms from BLR}{246}{subsection.6.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.1.1}Bayesian inference as optimization}{246}{subsubsection.6.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.1.2}Optimization of BLR using natural gradient descent}{247}{subsubsection.6.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.1.3}Conjugate variational inference}{248}{subsubsection.6.7.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.1.4}Partially conjugate variational inference}{248}{subsubsection.6.7.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.7.2}Deriving optimization algorithms from BLR}{248}{subsection.6.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.2.1}Gradient descent}{248}{subsubsection.6.7.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.2.2}Newton's method}{249}{subsubsection.6.7.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.2.3}Variational online Gauss-Newton}{250}{subsubsection.6.7.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.7.2.4}Adaptive learning rate SGD}{251}{subsubsection.6.7.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.7.3}Variational optimization}{251}{subsection.6.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.8}Bayesian optimization}{252}{section.6.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.8.1}Sequential model-based optimization}{253}{subsection.6.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.8.2}Surrogate functions}{253}{subsection.6.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.2.1}Gaussian processes}{254}{subsubsection.6.8.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.2.2}Bayesian neural networks}{255}{subsubsection.6.8.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.2.3}Other models}{255}{subsubsection.6.8.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.8.3}Acquisition functions}{255}{subsection.6.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.1}Probability of improvement}{255}{subsubsection.6.8.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.2}Expected improvement}{255}{subsubsection.6.8.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.3}Upper confidence bound (UCB)}{256}{subsubsection.6.8.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.4}Thompson sampling}{256}{subsubsection.6.8.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.5}Entropy search}{257}{subsubsection.6.8.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.6}Knowledge gradient}{257}{subsubsection.6.8.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.3.7}Optimizing the acquisition function}{258}{subsubsection.6.8.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.8.4}Other issues}{258}{subsection.6.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.4.1}Parallel (batch) queries}{258}{subsubsection.6.8.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.4.2}Conditional parameters}{258}{subsubsection.6.8.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.4.3}Multi-fidelity surrogates}{259}{subsubsection.6.8.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.8.4.4}Constraints}{259}{subsubsection.6.8.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.9}Derivative free optimization}{259}{section.6.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.1}Local search}{259}{subsection.6.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.1.1}Stochastic local search}{260}{subsubsection.6.9.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.1.2}Tabu search}{260}{subsubsection.6.9.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.1.3}Random search}{262}{subsubsection.6.9.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.2}Simulated annealing}{262}{subsection.6.9.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.3}Evolutionary algorithms}{265}{subsection.6.9.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.4}Estimation of distribution (EDA) algorithms}{266}{subsection.6.9.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.5}Cross-entropy method}{269}{subsection.6.9.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.5.1}Differentiable CEM}{269}{subsubsection.6.9.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.9.6}Evolutionary strategies}{269}{subsection.6.9.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.6.1}Natural evolutionary strategies}{270}{subsubsection.6.9.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.9.6.2}CMA-ES}{270}{subsubsection.6.9.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.10}Optimal Transport}{270}{section.6.10}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.10.1}Warm-up: Matching optimally two families of points}{271}{subsection.6.10.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.10.2}From Optimal Matchings to Kantorovich and Monge formulations}{271}{subsection.6.10.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.2.1}Mass splitting}{272}{subsubsection.6.10.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.2.2}Monge formulation and optimal push-forward maps}{273}{subsubsection.6.10.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.2.3}Kantorovich formulation}{273}{subsubsection.6.10.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.2.4}Wasserstein distances}{274}{subsubsection.6.10.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.10.3}Solving optimal transport}{274}{subsection.6.10.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.1}Duality and cost concavity}{274}{subsubsection.6.10.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.2}Kantorovich-Rubinstein duality and Lipschitz potentials}{275}{subsubsection.6.10.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.3}Monge maps as gradients of convex functions: the Brenier theorem}{275}{subsubsection.6.10.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.4}Closed forms for univariate and Gaussian distributions}{276}{subsubsection.6.10.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.5}Exact evaluation using linear program solvers}{277}{subsubsection.6.10.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.10.3.6}Obtaining smoothness using entropic regularization}{278}{subsubsection.6.10.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.11}Submodular optimization}{279}{section.6.11}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.1}Intuition, Examples, and Background}{279}{subsection.6.11.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.11.1.1}Coffee, Lemon, Milk, Tea}{280}{subsubsection.6.11.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.2}Submodular Basic Definitions}{282}{subsection.6.11.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.3}Example Submodular Functions}{283}{subsection.6.11.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.4}Submodular Optimization}{285}{subsection.6.11.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.11.4.1}Submodular Maximization}{287}{subsubsection.6.11.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.11.4.2}Discrete Constraints}{288}{subsubsection.6.11.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.11.4.3}Submodular Function Minimization}{289}{subsubsection.6.11.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.5}Applications of Submodularity in Machine Learning and AI}{290}{subsection.6.11.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.6}Sketching, CoreSets, Distillation, and Data Subset \& Feature Selection}{290}{subsection.6.11.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {6.11.6.1}Summarization Algorithm Design Choices}{291}{subsubsection.6.11.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.7}Combinatorial Information Functions}{294}{subsection.6.11.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.8}Clustering, Data Partitioning, and Parallel Machine Learning}{295}{subsection.6.11.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.9}Active and Semi-Supervised Learning}{295}{subsection.6.11.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.10}Probabilistic Modeling}{296}{subsection.6.11.10}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.11}Structured Norms and Loss Functions}{298}{subsection.6.11.11}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {6.11.12}Conclusions}{298}{subsection.6.11.12}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{II\hspace {1em}Inference}{299}{part.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {7}Inference algorithms: an overview}{301}{chapter.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {7.1}Introduction}{301}{section.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {7.2}Common inference patterns}{301}{section.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.2.1}Global latents}{302}{subsection.7.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.2.2}Local latents}{302}{subsection.7.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.2.3}Global and local latents}{303}{subsection.7.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {7.3}Exact inference algorithms}{303}{section.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {7.4}Approximate inference algorithms}{304}{section.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.1}MAP estimation}{304}{subsection.7.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.2}Grid approximation}{304}{subsection.7.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.3}Laplace (quadratic) approximation}{305}{subsection.7.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.4}Variational inference}{306}{subsection.7.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.5}Markov Chain Monte Carlo (MCMC)}{308}{subsection.7.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.6}Sequential Monte Carlo}{309}{subsection.7.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {7.4.7}Challenging posteriors}{310}{subsection.7.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {7.5}Evaluating approximate inference algorithms}{310}{section.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {8}Message passing inference}{313}{chapter.8}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.1}Introduction}{313}{section.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.2}Belief propagation for discrete chains}{314}{section.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.1}Example: casino HMM}{314}{subsection.8.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.2}Forwards filtering}{315}{subsection.8.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.2.1}Prediction step}{315}{subsubsection.8.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.2.2}Update step}{316}{subsubsection.8.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.2.3}Implementation for discrete state spaces}{316}{subsubsection.8.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.2.4}Example}{317}{subsubsection.8.2.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.3}Backwards smoothing}{317}{subsection.8.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.3.1}Backwards recursion}{317}{subsubsection.8.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.3.2}Implementation for discrete state spaces}{318}{subsubsection.8.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.3.3}Example}{318}{subsubsection.8.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.4}The forwards-backwards algorithm}{319}{subsection.8.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.4.1}Backwards recursion}{319}{subsubsection.8.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.4.2}Two-slice smoothed marginals}{320}{subsubsection.8.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.5}Time and space complexity}{320}{subsection.8.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.6}The Viterbi algorithm}{321}{subsection.8.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.6.1}Forwards pass}{322}{subsubsection.8.2.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.6.2}Backwards pass}{322}{subsubsection.8.2.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.6.3}Example}{323}{subsubsection.8.2.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.6.4}Time and space complexity}{324}{subsubsection.8.2.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.2.6.5}N-best list}{324}{subsubsection.8.2.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.2.7}Forwards filtering, backwards sampling}{324}{subsection.8.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.3}Belief propagation for Gaussian chains}{325}{section.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.3.1}Example: tracking SSM}{325}{subsection.8.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.3.2}The Kalman filter}{327}{subsection.8.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.2.1}Predict step}{327}{subsubsection.8.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.2.2}Update step}{328}{subsubsection.8.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.2.3}Posterior predictive}{328}{subsubsection.8.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.2.4}Steady state solution}{329}{subsubsection.8.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.3.3}The Kalman (RTS) smoother}{330}{subsection.8.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.3.1}Algorithm}{330}{subsubsection.8.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.3.2}Two-filter smoothing}{330}{subsubsection.8.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.3.3}Time and space complexity}{331}{subsubsection.8.3.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.3.3.4}Forwards filtering backwards sampling}{331}{subsubsection.8.3.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.4}Belief propagation for general chain-structured PGMs}{331}{section.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.4.1}Extended and unscented inference}{331}{subsection.8.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.4.2}Inference based on discretization}{331}{subsection.8.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.4.3}Particle filtering}{332}{subsection.8.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.4.4}Assumed density filtering}{333}{subsection.8.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.5}Belief propagation on trees}{334}{section.8.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.5.1}BP for polytrees}{334}{subsection.8.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.5.1.1}Computing the messages}{336}{subsubsection.8.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.5.1.2}Message passing protocol}{337}{subsubsection.8.5.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.5.2}BP for undirected graphs with pairwise potentials}{337}{subsection.8.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.5.3}Max product belief propagation}{338}{subsection.8.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.5.3.1}Connection between MMM and MAP}{338}{subsubsection.8.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.5.3.2}Connection between MPM and MAP}{339}{subsubsection.8.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.5.3.3}Connection between MPE and MAP}{339}{subsubsection.8.5.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.6}Loopy belief propagation}{340}{section.8.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.1}Loopy BP for factor graphs}{340}{subsection.8.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.2}Gaussian belief propagation}{341}{subsection.8.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.3}Convergence}{342}{subsection.8.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.6.3.1}When will LBP converge?}{343}{subsubsection.8.6.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.6.3.2}Making LBP converge}{343}{subsubsection.8.6.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.6.3.3}Increasing the convergence rate with adaptive scheduling}{344}{subsubsection.8.6.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.4}Accuracy}{345}{subsection.8.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.5}Connection with variational inference}{345}{subsection.8.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.6}Generalized belief propagation}{346}{subsection.8.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.7}Application: error correcting codes}{346}{subsection.8.6.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.8}Application: Affinity propagation}{347}{subsection.8.6.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.6.9}Emulating BP with graph neural nets}{349}{subsection.8.6.9}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.7}The variable elimination (VE) algorithm}{350}{section.8.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.7.1}Derivation of the algorithm}{350}{subsection.8.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.7.2}Computational complexity of VE}{351}{subsection.8.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.7.3}Computational complexity of exact inference}{353}{subsection.8.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.7.4}Drawbacks of VE}{354}{subsection.8.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.8}The junction tree algorithm (JTA)}{355}{section.8.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.8.1}Creating a junction tree}{355}{subsection.8.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.1.1}What is a tree decomposition?}{356}{subsubsection.8.8.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.1.2}Why create a tree decomposition?}{357}{subsubsection.8.8.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.1.3}Computing a tree decomposition}{357}{subsubsection.8.8.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.1.4}Picking a good elimination order}{358}{subsubsection.8.8.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.1.5}Computing a jtree from a directed graphical model}{360}{subsubsection.8.8.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.8.2}Message passing on a junction tree}{360}{subsection.8.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.2.1}Discrete potential functions}{361}{subsubsection.8.8.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.2.2}Initialization}{361}{subsubsection.8.8.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.2.3}Calibration}{362}{subsubsection.8.8.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.8.3}Gaussian message passing}{363}{subsection.8.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.1}Moment and canonical parameterization}{363}{subsubsection.8.8.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.2}Multiplication and division}{363}{subsubsection.8.8.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.3}Marginalization}{364}{subsubsection.8.8.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.4}Conditioning on evidence}{364}{subsubsection.8.8.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.5}Initialization}{364}{subsubsection.8.8.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.8.3.6}Product of Gaussians}{365}{subsubsection.8.8.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.8.4}The generalized distributive law}{365}{subsection.8.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {8.9}Inference as optimization}{366}{section.8.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.9.1}Inference as backpropagation}{367}{subsection.8.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.9.1.1}Example: inference in a small model}{367}{subsubsection.8.9.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.9.1.2}Example: Inference in an HMM}{368}{subsubsection.8.9.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {8.9.2}Perturb and MAP}{369}{subsection.8.9.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.9.2.1}Gaussian case}{369}{subsubsection.8.9.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {8.9.2.2}Discrete case}{370}{subsubsection.8.9.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {9}Variational inference}{371}{chapter.9}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.1}Introduction}{371}{section.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.1.1}Variational free energy}{371}{subsection.9.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.1.2}Evidence lower bound (ELBO)}{372}{subsection.9.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.2}Mean field VI}{373}{section.9.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.1}Coordinate ascent variational inference (CAVI)}{373}{subsection.9.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.2}Example: CAVI for the Ising model}{374}{subsection.9.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.3}Variational Bayes}{376}{subsection.9.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.4}Example: VB for a univariate Gaussian}{377}{subsection.9.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.1}Target distribution}{378}{subsubsection.9.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.2}Updating $q(\mu |\boldsymbol {\psi }_{\mu })$}{378}{subsubsection.9.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.3}Updating $q(\lambda |\boldsymbol {\psi }_{\lambda })$}{378}{subsubsection.9.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.4}Computing the expectations}{379}{subsubsection.9.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.5}Illustration}{379}{subsubsection.9.2.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.4.6}Lower bound}{379}{subsubsection.9.2.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.5}Variational Bayes EM}{380}{subsection.9.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.6}Example: VBEM for a GMM}{381}{subsection.9.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.1}The variational posterior}{382}{subsubsection.9.2.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.2}Derivation of $q({\bm {\theta }})$ (variational M step)}{382}{subsubsection.9.2.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.3}Derivation of $q({\bm {z}})$ (variational E step)}{383}{subsubsection.9.2.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.4}Automatic sparsity inducing effects of VBEM}{384}{subsubsection.9.2.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.5}Lower bound on the marginal likelihood}{386}{subsubsection.9.2.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.2.6.6}Model selection using VBEM}{387}{subsubsection.9.2.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.7}Variational message passing (VMP)}{387}{subsection.9.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.2.8}Autoconj}{388}{subsection.9.2.8}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.3}Fixed-form VI}{388}{section.9.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.1}Stochastic variational inference}{388}{subsection.9.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.2}Black-box variational inference}{389}{subsection.9.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.3}Reparameterization VI}{391}{subsection.9.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.3.1}``Sticking the landing'' estimator}{391}{subsubsection.9.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.4}Full-rank Gaussian VI}{392}{subsection.9.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.5}Low-rank Gaussian VI}{392}{subsection.9.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.5.1}Example: GVI for the mean of an MVN}{393}{subsubsection.9.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.5.2}Example: GVI for logistic regression}{394}{subsubsection.9.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.6}Automatic differentiation VI}{394}{subsection.9.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.1}Basic idea}{394}{subsubsection.9.3.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.2}Example: ADVI for Beta-Binomial model}{395}{subsubsection.9.3.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.3}Example: ADVI for the covariance of an MVN}{395}{subsubsection.9.3.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.4}Example: ADVI for GMMs}{396}{subsubsection.9.3.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.5}Example: ADVI for sparse Bayesian linear regression}{397}{subsubsection.9.3.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.6.6}More complex posteriors}{398}{subsubsection.9.3.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.7}Sparse Gaussian VI}{398}{subsection.9.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.7.1}Sparsity of the ELBO}{398}{subsubsection.9.3.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.7.2}Sparsity of the natural gradient of the ELBO}{399}{subsubsection.9.3.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.7.3}Computing posterior expectations}{399}{subsubsection.9.3.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.3.7.4}Gaussian VI for nonlinear least squares problems}{399}{subsubsection.9.3.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.8}Non-Gaussian reparameterized VI}{400}{subsection.9.3.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.3.9}Amortized inference}{401}{subsection.9.3.9}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.4}More accurate variational posteriors}{403}{section.9.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.4.1}Structured mean field}{403}{subsection.9.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.4.2}Hierarchical (auxiliary variable) posteriors}{403}{subsection.9.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.4.3}Normalizing flow posteriors}{404}{subsection.9.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.4.4}Implicit posteriors}{406}{subsection.9.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.4.5}Combining VI with MCMC inference}{406}{subsection.9.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.5}Lower bounds}{406}{section.9.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.5.1}Multi-sample ELBO (IWAE bound)}{406}{subsection.9.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.5.1.1}Pathologies of optimizing the IWAE bound}{407}{subsubsection.9.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.5.2}The thermodynamic variational objective (TVO)}{407}{subsection.9.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.6}Upper bounds}{408}{section.9.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.6.1}Minimizing the $\chi $-divergence upper bound}{409}{subsection.9.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.6.2}Minimizing the evidence upper bound}{410}{subsection.9.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {9.7}Expectation propagation (EP)}{410}{section.9.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.1}Minimizing forwards vs reverse KL}{410}{subsection.9.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.7.1.1}Moment projection}{411}{subsubsection.9.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {9.7.1.2}Information projection}{412}{subsubsection.9.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.2}EP as generalized ADF}{412}{subsection.9.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.3}Algorithm}{412}{subsection.9.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.4}Example}{414}{subsection.9.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.5}Optimization issues}{414}{subsection.9.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.6}Power EP and $\alpha $-divergence}{415}{subsection.9.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.7}Stochastic EP}{415}{subsection.9.7.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {9.7.8}Applications}{416}{subsection.9.7.8}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {10}Monte Carlo inference}{417}{chapter.10}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.1}Introduction}{417}{section.10.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.2}Monte Carlo integration}{417}{section.10.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.2.1}Example: estimating $\pi $ by Monte Carlo integration}{418}{subsection.10.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.2.2}Accuracy of Monte Carlo integration}{418}{subsection.10.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.3}Generating random samples from simple distributions}{420}{section.10.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.3.1}Sampling using the inverse cdf}{420}{subsection.10.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.3.2}Sampling from a Gaussian (Box-Muller method)}{421}{subsection.10.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.4}Rejection sampling}{421}{section.10.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.4.1}Basic idea}{422}{subsection.10.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.4.2}Example}{423}{subsection.10.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.4.3}Adaptive rejection sampling}{423}{subsection.10.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.4.4}Rejection sampling in high dimensions}{424}{subsection.10.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.5}Importance sampling}{424}{section.10.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.5.1}Direct importance sampling}{425}{subsection.10.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.5.2}Self-normalized importance sampling}{425}{subsection.10.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.5.3}Choosing the proposal}{426}{subsection.10.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.5.4}Annealed importance sampling (AIS)}{427}{subsection.10.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {10.5.4.1}Estimating normalizing constants using AIS}{428}{subsubsection.10.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {10.6}Controlling Monte Carlo variance}{428}{section.10.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.6.1}Common random numbers}{428}{subsection.10.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.6.2}Rao-Blackwellisation}{428}{subsection.10.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.6.3}Control variates}{429}{subsection.10.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {10.6.3.1}Example}{430}{subsubsection.10.6.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.6.4}Antithetic sampling}{430}{subsection.10.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {10.6.4.1}Example}{430}{subsubsection.10.6.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {10.6.5}Quasi Monte Carlo (QMC)}{431}{subsection.10.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {11}Markov Chain Monte Carlo inference}{433}{chapter.11}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.1}Introduction}{433}{section.11.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.2}Metropolis Hastings algorithm}{434}{section.11.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.2.1}Basic idea}{434}{subsection.11.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.2.2}Why MH works}{435}{subsection.11.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.2.3}Proposal distributions}{436}{subsection.11.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.2.3.1}Independence sampler}{436}{subsubsection.11.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.2.3.2}Random walk Metropolis (RWM) algorithm}{437}{subsubsection.11.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.2.3.3}Composing proposals}{438}{subsubsection.11.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.2.3.4}Data-driven MCMC}{438}{subsubsection.11.2.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.2.3.5}Adaptive MCMC}{438}{subsubsection.11.2.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.2.4}Initialization}{439}{subsection.11.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.3}Gibbs sampling}{439}{section.11.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.1}Basic idea}{439}{subsection.11.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.2}Gibbs sampling is a special case of MH}{440}{subsection.11.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.3}Example: Gibbs sampling for Ising models}{440}{subsection.11.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.4}Example: Gibbs sampling for Potts models}{442}{subsection.11.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.5}Example: Gibbs sampling for GMMs}{442}{subsection.11.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.5.1}Known parameters}{442}{subsubsection.11.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.5.2}Unknown parameters}{444}{subsubsection.11.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.6}Sampling from the full conditionals}{444}{subsection.11.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.6.1}Adaptive rejection Metropolis sampling}{445}{subsubsection.11.3.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.6.2}Metropolis within Gibbs}{445}{subsubsection.11.3.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.7}Blocked Gibbs sampling}{445}{subsection.11.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.7.1}Example: Blocked Gibbs for HMMs}{446}{subsubsection.11.3.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.3.8}Collapsed Gibbs sampling}{446}{subsection.11.3.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.8.1}Example: collapsed Gibbs for GMMs}{446}{subsubsection.11.3.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.3.8.2}Example: Collapsed Gibbs sampling for LDA}{448}{subsubsection.11.3.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.4}Auxiliary variable MCMC}{448}{section.11.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.4.1}Slice sampling}{449}{subsection.11.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.4.2}Swendsen Wang}{450}{subsection.11.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.5}Hamiltonian Monte Carlo (HMC)}{452}{section.11.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.1}Hamiltonian mechanics}{452}{subsection.11.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.2}Integrating Hamilton's equations}{453}{subsection.11.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.2.1}Euler's method}{453}{subsubsection.11.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.2.2}Modified Euler's method}{453}{subsubsection.11.5.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.2.3}Leapfrog integrator}{454}{subsubsection.11.5.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.2.4}Higher order integrators}{454}{subsubsection.11.5.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.3}The HMC algorithm}{454}{subsection.11.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.4}Tuning HMC}{455}{subsection.11.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.4.1}Choosing the number of steps using NUTS}{455}{subsubsection.11.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.4.2}Choosing the step size}{455}{subsubsection.11.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.5.4.3}Choosing the covariance (inverse mass) matrix}{456}{subsubsection.11.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.5}Riemann Manifold HMC}{456}{subsection.11.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.6}Langevin Monte Carlo (MALA)}{457}{subsection.11.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.7}Connection between SGD and Langevin sampling}{458}{subsection.11.5.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.8}Applying HMC to constrained parameters}{460}{subsection.11.5.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.5.9}Speeding up HMC}{461}{subsection.11.5.9}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.6}MCMC convergence}{461}{section.11.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.6.1}Mixing rates of Markov chains}{462}{subsection.11.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.6.2}Practical convergence diagnostics}{462}{subsection.11.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.6.2.1}Trace plots}{463}{subsubsection.11.6.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.6.2.2}Estimated potential scale reduction (EPSR)}{464}{subsubsection.11.6.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.6.2.3}Effective sample size}{467}{subsubsection.11.6.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {11.6.2.4}Estimating the ACF from multiple chains using variograms}{469}{subsubsection.11.6.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.6.3}Improving speed of convergence}{470}{subsection.11.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.6.4}Non-centered parameterizations and Neal's funnel}{470}{subsection.11.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.7}Stochastic gradient MCMC}{471}{section.11.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.7.1}Stochastic Gradient Langevin Dynamics (SGLD)}{472}{subsection.11.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.7.2}Preconditionining}{472}{subsection.11.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.7.3}Reducing the variance of the gradient estimate}{473}{subsection.11.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.7.4}SG-HMC}{474}{subsection.11.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.7.5}Underdamped Langevin Dynamics}{475}{subsection.11.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.8}Reversible jump (trans-dimensional) MCMC}{476}{section.11.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.8.1}Basic idea}{476}{subsection.11.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.8.2}Example}{478}{subsection.11.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.8.3}Discussion}{479}{subsection.11.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {11.9}Annealing methods}{479}{section.11.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {11.9.1}Parallel tempering}{480}{subsection.11.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {12}Sequential Monte Carlo inference}{481}{chapter.12}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {12.1}Introduction}{481}{section.12.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.1.1}Problem statement}{481}{subsection.12.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.1.2}Particle filtering for state-space models}{481}{subsection.12.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.1.3}SMC samplers for static parameter estimation}{483}{subsection.12.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {12.2}Particle filtering}{483}{section.12.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.1}Importance sampling}{483}{subsection.12.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.2}Sequential importance sampling}{484}{subsection.12.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.3}Sequential importance sampling with resampling}{485}{subsection.12.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.3.1}Bootstrap filter}{487}{subsubsection.12.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.3.2}Estimating the normalizing constant}{487}{subsubsection.12.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.3.3}Path degeneracy problem}{488}{subsubsection.12.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.4}Resampling methods}{488}{subsection.12.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.4.1}Multinomial resampling}{488}{subsubsection.12.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.4.2}Stratified resampling}{489}{subsubsection.12.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.4.3}Systematic resampling}{490}{subsubsection.12.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.2.4.4}Comparison}{490}{subsubsection.12.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.5}Adaptive resampling}{490}{subsection.12.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.6}Proposal distributions}{491}{subsection.12.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.2.7}Example: Robot localization}{491}{subsection.12.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {12.3}SMC samplers}{492}{section.12.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.1}Ingredients of an SMC sampler}{493}{subsection.12.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.2}Likelihood tempering (geometric path)}{494}{subsection.12.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.3.2.1}Example: sampling from a 1d bimodal distribution}{495}{subsubsection.12.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.3}Data tempering}{497}{subsection.12.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {12.3.3.1}Example: IBIS for a 1d Gaussian}{498}{subsubsection.12.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.4}Sampling rare events and extrema}{498}{subsection.12.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.5}SMC-ABC and likelihood-free inference}{499}{subsection.12.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.3.6}SMC$^2$}{499}{subsection.12.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {12.4}Particle MCMC methods}{500}{section.12.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.4.1}Particle Marginal Metropolis Hastings}{500}{subsection.12.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.4.2}Particle Independent Metropolis Hastings}{501}{subsection.12.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {12.4.3}Particle Gibbs}{502}{subsection.12.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{III\hspace {1em}Prediction}{503}{part.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {13}Predictive models: an overview}{505}{chapter.13}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {13.1}Introduction}{505}{section.13.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.1.1}Types of model}{505}{subsection.13.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.1.2}Model fitting using ERM, MLE and MAP}{506}{subsection.13.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.1.3}Model fitting using Bayes, VI and generalized Bayes}{507}{subsection.13.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {13.2}Evaluating predictive models}{508}{section.13.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.2.1}Proper scoring rules}{508}{subsection.13.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.2.2}Calibration}{508}{subsection.13.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.1}Expected calibration error}{508}{subsubsection.13.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.2}Improving calibration}{510}{subsubsection.13.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.3}Platt scaling}{510}{subsubsection.13.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.4}Nonparametric (histogram) methods}{510}{subsubsection.13.2.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.5}Temperature scaling}{510}{subsubsection.13.2.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.6}Label smoothing}{511}{subsubsection.13.2.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.2.7}Bayesian methods}{512}{subsubsection.13.2.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.2.3}Beyond evaluating marginal probabilities}{512}{subsection.13.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.2.3.1}Proof of claim}{514}{subsubsection.13.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {13.3}Conformal prediction}{515}{section.13.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.3.1}Conformalizing classification}{516}{subsection.13.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.3.2}Conformalizing regression}{517}{subsection.13.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.3.2.1}Conformalizing quantile regression}{517}{subsubsection.13.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {13.3.2.2}Conformalizing predicted variances}{518}{subsubsection.13.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.3.3}Conformalizing Bayes}{518}{subsection.13.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {13.3.4}What do we do if we don't have a calibration set?}{519}{subsection.13.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {14}Generalized linear models}{521}{chapter.14}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {14.1}Introduction}{521}{section.14.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.1.1}Examples}{521}{subsection.14.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.1.1.1}Linear regression}{521}{subsubsection.14.1.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.1.1.2}Binomial regression}{522}{subsubsection.14.1.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.1.1.3}Poisson regression}{523}{subsubsection.14.1.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.1.1.4}Zero-inflated Poisson regression}{523}{subsubsection.14.1.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.1.2}GLMs with non-canonical link functions}{524}{subsection.14.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.1.3}Maximum likelihood estimation}{524}{subsection.14.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.1.4}Bayesian inference}{525}{subsection.14.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {14.2}Linear regression}{526}{section.14.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.1}Conjugate priors}{526}{subsection.14.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.1.1}Noise variance is known}{526}{subsubsection.14.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.1.2}Noise variance is unknown}{527}{subsubsection.14.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.1.3}Posterior predictive distribution}{528}{subsubsection.14.2.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.2}Uninformative priors}{528}{subsection.14.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.2.1}Jeffreys prior}{528}{subsubsection.14.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.2.2}Connection to frequentist statistics}{529}{subsubsection.14.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.2.3}Zellner's $g$-prior}{529}{subsubsection.14.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.3}Informative priors}{530}{subsection.14.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.4}Spike and slab prior}{532}{subsection.14.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.5}Laplace prior (Bayesian lasso)}{533}{subsection.14.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.6}Horseshoe prior}{534}{subsection.14.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.2.7}Automatic relevancy determination}{535}{subsection.14.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.7.1}ARD for linear models}{535}{subsubsection.14.2.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.7.2}Why does ARD result in a sparse solution?}{536}{subsubsection.14.2.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.7.3}Algorithms for ARD}{537}{subsubsection.14.2.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.2.7.4}Relevance vector machines}{537}{subsubsection.14.2.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {14.3}Logistic regression}{537}{section.14.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.1}Binary logistic regression}{538}{subsection.14.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.2}Multinomial logistic regression}{538}{subsection.14.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.3}Priors}{539}{subsection.14.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.4}Posteriors}{540}{subsection.14.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.5}Laplace approximation}{540}{subsection.14.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.6}MCMC inference}{543}{subsection.14.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.7}Variational inference}{544}{subsection.14.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.3.8}Assumed density filtering}{544}{subsection.14.3.8}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {14.4}Probit regression}{547}{section.14.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.4.1}Latent variable interpretation}{547}{subsection.14.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.4.2}Maximum likelihood estimation}{548}{subsection.14.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.4.2.1}MLE using SGD}{548}{subsubsection.14.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.4.2.2}MLE using EM}{549}{subsubsection.14.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.4.3}Bayesian inference}{550}{subsection.14.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.4.4}Ordinal probit regression}{550}{subsection.14.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.4.5}Multinomial probit models }{551}{subsection.14.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {14.5}Multi-level GLMs}{551}{section.14.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.5.1}Generalized linear mixed models (GLMMs)}{551}{subsection.14.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.5.2}Model fitting}{552}{subsection.14.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {14.5.3}Example: radon regression}{552}{subsection.14.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.5.3.1}Posterior inference}{554}{subsubsection.14.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {14.5.3.2}Non-centered parameterization}{554}{subsubsection.14.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {15}Deep neural networks}{557}{chapter.15}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {15.1}Introduction}{557}{section.15.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {15.2}Building blocks of differentiable circuits}{557}{section.15.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.1}Linear layers}{558}{subsection.15.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.2}Non-linearities}{558}{subsection.15.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.3}Convolutional layers}{559}{subsection.15.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.4}Residual (skip) connections}{560}{subsection.15.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.5}Normalization layers}{561}{subsection.15.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.6}Dropout layers}{561}{subsection.15.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.7}Attention layers}{562}{subsection.15.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.8}Recurrent layers}{565}{subsection.15.2.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.9}Multiplicative layers}{565}{subsection.15.2.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.2.10}Implicit layers}{566}{subsection.15.2.10}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {15.3}Canonical examples of neural networks}{566}{section.15.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.3.1}Multi-layer perceptrons (MLP)}{567}{subsection.15.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.3.2}Convolutional neural networks (CNN)}{567}{subsection.15.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.3.3}Recurrent neural networks (RNN)}{567}{subsection.15.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.3.4}Transformers}{569}{subsection.15.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.4.1}Encoder}{570}{subsubsection.15.3.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.4.2}Decoder}{570}{subsubsection.15.3.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.4.3}Putting it all together}{571}{subsubsection.15.3.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.4.4}Discussion}{571}{subsubsection.15.3.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {15.3.5}Graph neural networks (GNNs)}{572}{subsection.15.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.5.1}Basics of GNNs}{573}{subsubsection.15.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.5.2}Message passing}{574}{subsubsection.15.3.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.5.3}More complex types of graphs}{575}{subsubsection.15.3.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.5.4}Graph attention networks}{575}{subsubsection.15.3.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {15.3.5.5}Transformers are fully connected GNNs}{575}{subsubsection.15.3.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {16}Bayesian neural networks}{579}{chapter.16}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.1}Introduction}{579}{section.16.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.2}Priors for BNNs}{579}{section.16.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.2.1}Gaussian priors}{579}{subsection.16.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.2.2}Sparsity-promoting priors}{582}{subsection.16.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.2.3}Learning the prior}{582}{subsection.16.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.2.4}Priors in function space}{582}{subsection.16.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.2.5}Architectural priors}{582}{subsection.16.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.3}Likelihoods for BNNs}{583}{section.16.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.4}Posteriors for BNNs}{584}{section.16.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.1}Laplace approximation}{584}{subsection.16.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.2}Variational inference}{585}{subsection.16.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.3}Expectation propagation}{586}{subsection.16.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.4}Last layer methods}{586}{subsection.16.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.5}Dropout}{587}{subsection.16.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.6}MCMC methods}{587}{subsection.16.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.7}Methods based on the SGD trajectory}{587}{subsection.16.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.8}Deep ensembles}{588}{subsection.16.4.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.8.1}Multi-SWAG}{589}{subsubsection.16.4.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.8.2}Deep ensembles with random priors}{590}{subsubsection.16.4.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.8.3}Deep ensembles as approximate Bayesian inference}{590}{subsubsection.16.4.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.8.4}Batch ensemble}{592}{subsubsection.16.4.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.4.9}Approximating the posterior predictive distibution}{593}{subsection.16.4.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.9.1}A linearized approximation}{593}{subsubsection.16.4.9.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.4.9.2}Distillation}{593}{subsubsection.16.4.9.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.5}Generalization in Bayesian deep learning}{594}{section.16.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.1}Sharp vs flat minima}{594}{subsection.16.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.2}Effective dimensionality of a model}{595}{subsection.16.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.3}The hypothesis space of DNNs}{597}{subsection.16.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.4}Double descent}{598}{subsection.16.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.5}A Bayesian Resolution to Double Descent}{599}{subsection.16.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.6}PAC-Bayes}{601}{subsection.16.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.5.7}Out-of-Distribution Generalization for BNNs}{602}{subsection.16.5.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.6}Online inference}{605}{section.16.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.6.1}Extended Kalman Filtering for DNNs}{605}{subsection.16.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.6.1.1}Example}{606}{subsubsection.16.6.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.6.1.2}Setting the variance terms}{607}{subsubsection.16.6.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.6.1.3}Reducing the computational complexity}{607}{subsubsection.16.6.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {16.6.1.4}Beyond squared error}{607}{subsubsection.16.6.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.6.2}Assumed Density Filtering for DNNs}{607}{subsection.16.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.6.3}Sequential Laplace for DNNs}{609}{subsection.16.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.6.4}Variational methods}{609}{subsection.16.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {16.7}Hierarchical Bayesian neural networks}{609}{section.16.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {16.7.1}Solving multiple related classification problems}{610}{subsection.16.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {17}Gaussian processes}{613}{chapter.17}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.1}Introduction}{613}{section.17.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.1.1}GPs: What and why?}{613}{subsection.17.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.2}Mercer kernels}{615}{section.17.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.2.1}Some popular Mercer kernels}{616}{subsection.17.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.2.1.1}Stationary kernels for real-valued vectors}{616}{subsubsection.17.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.2.1.2}Making new kernels from old}{620}{subsubsection.17.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.2.1.3}Combining kernels by addition and multiplication}{620}{subsubsection.17.2.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.2.1.4}Kernels for structured inputs}{622}{subsubsection.17.2.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.2.2}Mercer's theorem}{622}{subsection.17.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.2.3}Kernels from Spectral Densities}{623}{subsection.17.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.2.3.1}Random feature kernels}{624}{subsubsection.17.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.3}GPs with Gaussian likelihoods}{624}{section.17.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.1}Predictions using noise-free observations}{624}{subsection.17.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.2}Predictions using noisy observations}{626}{subsection.17.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.3}Weight space vs function space}{627}{subsection.17.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.4}Semi-parametric GPs}{627}{subsection.17.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.5}Marginal likelihood}{628}{subsection.17.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.6}Computational and numerical issues}{629}{subsection.17.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.3.7}Kernel ridge regression}{629}{subsection.17.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.3.7.1}Reproducing kernel Hilbert spaces}{630}{subsubsection.17.3.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.3.7.2}Complexity of a function in an RKHS}{631}{subsubsection.17.3.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.3.7.3}Representer theorem}{631}{subsubsection.17.3.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.3.7.4}Example of KRR vs GPR}{632}{subsubsection.17.3.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.4}GPs with non-Gaussian likelihoods}{632}{section.17.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.4.1}Binary classification}{633}{subsection.17.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.4.2}Multi-class classification}{634}{subsection.17.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.4.3}GPs for Poisson regression (Cox process)}{635}{subsection.17.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.4.4}Other likelihoods}{635}{subsection.17.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.5}Scaling GP inference to large datasets}{636}{section.17.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.1}Subset of data}{636}{subsection.17.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.1.1}Informative vector machine}{637}{subsubsection.17.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.1.2}Discussion}{637}{subsubsection.17.5.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.2}Nystr{\"o}m\xspace approximation}{637}{subsection.17.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.3}Inducing point methods}{638}{subsection.17.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.3.1}SOR/ DIC}{639}{subsubsection.17.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.3.2}DTC}{640}{subsubsection.17.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.3.3}FITC}{640}{subsubsection.17.5.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.3.4}Learning the inducing points}{641}{subsubsection.17.5.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.4}Sparse variational methods}{641}{subsection.17.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.4.1}Gaussian likelihood}{643}{subsubsection.17.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.4.2}Non-Gaussian likelihood}{644}{subsubsection.17.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.4.3}Minibatch SVI}{645}{subsubsection.17.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.5}Exploiting parallelization and structure via kernel matrix multiplies}{645}{subsection.17.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.5.1}Using conjugate gradient and Lanczos methods}{645}{subsubsection.17.5.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.5.2}Kernels with compact support}{646}{subsubsection.17.5.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.5.3}KISS}{646}{subsubsection.17.5.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.5.5.4}Tensor train methods}{647}{subsubsection.17.5.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.5.6}Converting a GP to a SSM}{647}{subsection.17.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.6}Learning the kernel}{647}{section.17.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.1}Empirical Bayes for the kernel parameters}{648}{subsection.17.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.6.1.1}Example}{649}{subsubsection.17.6.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.2}Bayesian inference for the kernel parameters}{650}{subsection.17.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.3}Multiple kernel learning for additive kernels}{651}{subsection.17.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.4}Automatic search for compositional kernels}{653}{subsection.17.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.5}Spectral mixture kernel learning}{655}{subsection.17.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.6}Deep kernel learning}{657}{subsection.17.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.6.7}Functional kernel learning}{659}{subsection.17.6.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {17.7}GPs and DNNs}{659}{section.17.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.7.1}Kernels derived from random DNNs (NN-GP)}{659}{subsection.17.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.1.1}Result for MLP with one hidden layer}{660}{subsubsection.17.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.1.2}Result for deep MLPs}{661}{subsubsection.17.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.7.2}Kernels derived from trained DNNs (neural tangent kernel)}{663}{subsection.17.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.2.1}Computing the NTK}{664}{subsubsection.17.7.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.2.2}Connections with GPs}{664}{subsubsection.17.7.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.2.3}Discussion}{665}{subsubsection.17.7.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {17.7.3}Deep GPs}{665}{subsection.17.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.3.1}Construction of a deep GP}{665}{subsubsection.17.7.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.3.2}Example: 1d step function}{665}{subsubsection.17.7.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.3.3}Posterior inference}{667}{subsubsection.17.7.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.3.4}Behavior in the limit of infinite width}{668}{subsubsection.17.7.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {17.7.3.5}Connection with Bayesian neural networks}{669}{subsubsection.17.7.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {18}Beyond the iid assumption}{671}{chapter.18}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.1}Introduction}{671}{section.18.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.2}Distribution shift}{671}{section.18.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.1}Motivating examples}{671}{subsection.18.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.2}A causal view of distribution shift}{673}{subsection.18.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.3}Covariate shift}{673}{subsection.18.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.4}Domain shift}{674}{subsection.18.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.5}Label / prior shift}{675}{subsection.18.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.6}Concept shift}{675}{subsection.18.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.7}Manifestation shift}{675}{subsection.18.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.2.8}Selection bias}{676}{subsection.18.2.8}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.3}Training-time techniques for distribution shift}{676}{section.18.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.1}Importance weighting for covariate shift}{677}{subsection.18.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.3.1.1}Conformal prediction with covariate shift}{677}{subsubsection.18.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.2}Domain adaptation}{678}{subsection.18.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.3}Domain randomization}{678}{subsection.18.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.4}Data augmentation}{679}{subsection.18.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.5}Unsupervised label shift estimation}{679}{subsection.18.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.3.6}Distributionally robust optimization}{680}{subsection.18.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.4}Test-time techniques for distribution shift}{680}{section.18.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.4.1}Detecting shifts using two-sample testing}{680}{subsection.18.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.4.2}Detecting single out-of-distribution (OOD) inputs}{680}{subsection.18.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.4.2.1}Supervised ID/OOD methods (outlier exposure)}{681}{subsubsection.18.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.4.2.2}Classification confidence methods}{681}{subsubsection.18.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.4.2.3}Conformal prediction}{682}{subsubsection.18.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.4.2.4}Unsupervised methods}{682}{subsubsection.18.4.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.4.3}Selective prediction}{683}{subsection.18.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.4.3.1}Example: SGLD vs SGD for MLPs}{683}{subsubsection.18.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.4.4}Open world recognition}{684}{subsection.18.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.4.5}Online adaptation}{685}{subsection.18.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.5}Learning from multiple distributions}{686}{section.18.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.1}Transfer learning}{687}{subsection.18.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {18.5.1.1}Pre-train and fine-tune}{687}{subsubsection.18.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.2}Few-shot learning}{688}{subsection.18.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.3}Prompt tuning}{688}{subsection.18.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.4}Zero-shot learning}{688}{subsection.18.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.5}Multi-task learning}{689}{subsection.18.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.6}Domain generalization}{690}{subsection.18.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.5.7}Invariant risk minimization}{691}{subsection.18.5.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.6}Meta-learning}{692}{section.18.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.6.1}Meta-learning as probabilistic inference for prediction}{693}{subsection.18.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.6.2}Gradient-based meta-learning}{694}{subsection.18.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.6.3}Metric-based few-shot learning}{694}{subsection.18.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.6.4}VERSA}{694}{subsection.18.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.6.5}Neural processes}{695}{subsection.18.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.7}Continual learning}{695}{section.18.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.7.1}Domain drift}{695}{subsection.18.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.7.2}Concept drift}{695}{subsection.18.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.7.3}Task incremental learning}{697}{subsection.18.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.7.4}Catastrophic forgetting}{698}{subsection.18.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.7.5}Online learning}{700}{subsection.18.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {18.8}Adversarial examples}{701}{section.18.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.8.1}Whitebox (gradient-based) attacks}{703}{subsection.18.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.8.2}Blackbox (gradient-free) attacks}{703}{subsection.18.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.8.3}Real world adversarial attacks}{705}{subsection.18.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.8.4}Defenses based on robust optimization}{705}{subsection.18.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {18.8.5}Why models have adversarial examples}{706}{subsection.18.8.5}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{IV\hspace {1em}Generation}{709}{part.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {19}Generative models: an overview}{711}{chapter.19}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {19.1}Introduction}{711}{section.19.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {19.2}Types of generative model}{711}{section.19.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {19.3}Goals of generative modeling}{713}{section.19.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.1}Generating data}{713}{subsection.19.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.2}Density estimation}{714}{subsection.19.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.3}Imputation}{714}{subsection.19.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.4}Structure discovery}{716}{subsection.19.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.5}Latent space interpolation}{716}{subsection.19.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.3.6}Representation learning}{717}{subsection.19.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {19.4}Evaluating generative models}{717}{section.19.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.1}Likelihood}{718}{subsection.19.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {19.4.1.1}Evaluating log-likelihood}{719}{subsubsection.19.4.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {19.4.1.2}Challenges with using likelihood}{719}{subsubsection.19.4.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {19.4.1.3}Likelihood can be hard to compute}{719}{subsubsection.19.4.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {19.4.1.4}Likelihood is not related to sample quality}{719}{subsubsection.19.4.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.2}Distances and divergences in feature space}{720}{subsection.19.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.3}Precision and recall metrics}{721}{subsection.19.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.4}Statistical tests}{722}{subsection.19.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.5}Challenges with using pretrained classifiers}{722}{subsection.19.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.6}Using model samples to train classifiers}{723}{subsection.19.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.7}Assessing overfitting}{723}{subsection.19.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {19.4.8}Human evaluation}{724}{subsection.19.4.8}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {20}Variational autoencoders}{725}{chapter.20}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.1}Introduction}{725}{section.20.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.2}VAE basics}{725}{section.20.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.1}Modeling assumptions}{726}{subsection.20.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.2}Evidence lower bound}{727}{subsection.20.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.3}Optimization}{728}{subsection.20.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.4}The reparameterization trick}{728}{subsection.20.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.5}Computing the reparameterized ELBO}{730}{subsection.20.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.2.5.1}Fully factorized Gaussian}{730}{subsubsection.20.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.2.5.2}Full covariance Gaussian}{730}{subsubsection.20.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.2.5.3}Inverse autoregressive flows}{731}{subsubsection.20.2.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.6}Comparison of VAEs and autoencoders}{732}{subsection.20.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.2.7}VAEs optimize in an augmented space}{733}{subsection.20.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.3}VAE generalizations}{734}{section.20.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.1}$\sigma $-VAE}{734}{subsection.20.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.2}$\beta $-VAE}{736}{subsection.20.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.2.1}Connection with $\sigma $-VAE}{737}{subsubsection.20.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.2.2}Disentangled representations}{737}{subsubsection.20.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.2.3}Connection with information bottleneck}{738}{subsubsection.20.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.3}InfoVAE}{738}{subsection.20.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.3.1}Connection with MMD VAE}{739}{subsubsection.20.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.3.2}Connection with $\beta $-VAEs}{740}{subsubsection.20.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.3.3}Connection with adversarial autoencoders}{740}{subsubsection.20.3.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.3.4}Example}{740}{subsubsection.20.3.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.4}Multi-modal VAEs}{740}{subsection.20.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.5}VAEs with missing data}{743}{subsection.20.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.6}Semi-supervised VAEs}{745}{subsection.20.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.3.7}VAEs with sequential encoders/decoders}{746}{subsection.20.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.7.1}Models}{746}{subsubsection.20.3.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {20.3.7.2}Applications}{747}{subsubsection.20.3.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.4}Avoiding posterior collapse}{749}{section.20.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.1}KL annealing}{750}{subsection.20.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.2}Lower bounding the rate}{750}{subsection.20.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.3}Free bits}{750}{subsection.20.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.4}Adding skip connections}{751}{subsection.20.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.5}Improved variational inference}{751}{subsection.20.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.6}Alternative objectives}{751}{subsection.20.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.4.7}Enforcing identifiability}{752}{subsection.20.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.5}VAEs with hierarchical structure}{753}{section.20.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.5.1}Bottom-up vs top-down inference}{753}{subsection.20.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.5.2}Example: Very deep VAE}{754}{subsection.20.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.5.3}Connection with autoregressive models}{755}{subsection.20.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.5.4}Variational pruning}{757}{subsection.20.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.5.5}Other optimization difficulties}{758}{subsection.20.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.6}Vector quantization VAE}{758}{section.20.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.1}Autoencoder with binary code}{758}{subsection.20.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.2}VQ-VAE model}{760}{subsection.20.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.3}Learning the prior}{761}{subsection.20.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.4}Hierarchical extension (VQ-VAE-2)}{761}{subsection.20.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.5}Discrete VAE}{762}{subsection.20.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.6.6}VQ-GAN}{763}{subsection.20.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {20.7}Wake-sleep algorithm}{764}{section.20.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.7.1}Wake phase}{765}{subsection.20.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.7.2}Sleep phase}{766}{subsection.20.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.7.3}Daydream phase}{766}{subsection.20.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {20.7.4}Summary of algorithm}{767}{subsection.20.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {21}Auto-regressive models}{769}{chapter.21}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {21.1}Introduction}{769}{section.21.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {21.2}Neural autoregressive density estimators (NADE)}{770}{section.21.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {21.3}Causal CNNs}{770}{section.21.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {21.3.1}1d causal CNN (Convolutional Markov models)}{771}{subsection.21.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {21.3.2}2d causal CNN (PixelCNN)}{771}{subsection.21.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {21.4}Transformer decoders}{772}{section.21.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {21.4.1}Text generation (GPT)}{773}{subsection.21.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {21.4.2}Music generation}{773}{subsection.21.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {21.4.3}Text-to-image generation (DALL-E)}{774}{subsection.21.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {22}Normalizing Flows}{777}{chapter.22}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {22.1}Introduction}{777}{section.22.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.1.1}Preliminaries}{777}{subsection.22.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.1.2}Example }{779}{subsection.22.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.1.3}How to train a flow model}{780}{subsection.22.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.1.3.1}Density estimation}{780}{subsubsection.22.1.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.1.3.2}Variational inference}{780}{subsubsection.22.1.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {22.2}Constructing Flows}{781}{section.22.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.1}Affine flows}{781}{subsection.22.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.2}Elementwise flows}{782}{subsection.22.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.2.1}Affine scalar bijection}{782}{subsubsection.22.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.2.2}Higher-order perturbations}{782}{subsubsection.22.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.2.3}Combinations of strictly monotonic scalar functions}{783}{subsubsection.22.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.2.4}Scalar bijections from integration}{783}{subsubsection.22.2.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.2.5}Splines}{784}{subsubsection.22.2.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.3}Coupling flows}{784}{subsection.22.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.4}Autoregressive flows}{786}{subsection.22.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.4.1}Affine autoregressive flows}{787}{subsubsection.22.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.4.2}Masked autoregressive flows}{788}{subsubsection.22.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.4.3}Inverse autoregressive flows}{789}{subsubsection.22.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.4.4}Connection with autoregressive models}{790}{subsubsection.22.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.5}Residual flows}{791}{subsection.22.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.5.1}Contractive residual blocks}{791}{subsubsection.22.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {22.2.5.2}Residual blocks with low-rank Jacobian}{792}{subsubsection.22.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.2.6}Continuous-time flows}{793}{subsection.22.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {22.3}Applications}{795}{section.22.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.3.1}Density estimation}{795}{subsection.22.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.3.2}Generative Modeling}{796}{subsection.22.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {22.3.3}Inference}{796}{subsection.22.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {23}Energy-based models}{799}{chapter.23}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {23.1}Introduction}{799}{section.23.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.1.1}Example: Products of experts (PoE)}{800}{subsection.23.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.1.2}Computational difficulties}{800}{subsection.23.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {23.2}Maximum Likelihood Training}{801}{section.23.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.2.1}Gradient-based MCMC methods}{802}{subsection.23.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.2.2}Contrastive divergence}{802}{subsection.23.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {23.2.2.1}Fitting RBMs with CD}{803}{subsubsection.23.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {23.2.2.2}Persistent CD}{805}{subsubsection.23.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {23.2.2.3}Other methods}{805}{subsubsection.23.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {23.3}Score Matching (SM)}{805}{section.23.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.3.1}Basic score matching}{806}{subsection.23.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.3.2}Denoising Score Matching (DSM)}{807}{subsection.23.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {23.3.2.1}Difficulties}{807}{subsubsection.23.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.3.3}Sliced Score Matching (SSM)}{808}{subsection.23.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.3.4}Connection to Contrastive Divergence}{809}{subsection.23.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.3.5}Score-Based Generative Models}{810}{subsection.23.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {23.3.5.1}Adding noise at multiple scales}{810}{subsubsection.23.3.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {23.4}Noise Contrastive Estimation}{813}{section.23.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.4.1}Connection to Score Matching}{814}{subsection.23.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {23.5}Other Methods}{815}{section.23.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.5.1}Minimizing Differences/Derivatives of KL Divergences}{815}{subsection.23.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.5.2}Minimizing the Stein Discrepancy}{816}{subsection.23.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {23.5.3}Adversarial Training}{816}{subsection.23.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {24}Denoising diffusion models}{819}{chapter.24}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {24.1}Model definition}{819}{section.24.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {24.2}Examples}{821}{section.24.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {24.3}Model training}{822}{section.24.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {24.4}Connections with other generative models}{824}{section.24.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {24.4.1}Connection with score matching}{824}{subsection.24.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {24.4.2}Connection with VAEs}{825}{subsection.24.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {24.4.3}Connection with flow models}{825}{subsection.24.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {25}Generative adversarial networks}{827}{chapter.25}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.1}Introduction}{827}{section.25.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.2}Learning by Comparison}{828}{section.25.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.1}Guiding principles}{829}{subsection.25.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.2}Class probability estimation}{830}{subsection.25.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.3}Bounds on $f$-divergences}{833}{subsection.25.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.4}Integral probability metrics}{834}{subsection.25.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.5}Moment matching}{836}{subsection.25.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.2.6}On density ratios and differences}{837}{subsection.25.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.3}Generative Adversarial Networks}{838}{section.25.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.3.1}From learning principles to loss functions}{839}{subsection.25.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.3.2}Gradient Descent}{840}{subsection.25.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.3.3}Challenges with GAN training}{841}{subsection.25.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.3.4}Improving GAN optimization}{843}{subsection.25.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.3.5}Convergence of GAN training}{843}{subsection.25.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.4}Conditional GANs}{847}{section.25.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.5}Inference with GANs}{848}{section.25.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.6}Neural architectures in GANs}{849}{section.25.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.1}The importance of discriminator architectures}{849}{subsection.25.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.2}Architectural inductive biases}{849}{subsection.25.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.3}Attention in GANs}{850}{subsection.25.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.4}Progressive generation}{851}{subsection.25.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.5}Regularization}{852}{subsection.25.6.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.6.6}Scaling up GAN models}{853}{subsection.25.6.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {25.7}Applications}{853}{section.25.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.1}GANs for image generation}{853}{subsection.25.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {25.7.1.1}Conditional image generation}{853}{subsubsection.25.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {25.7.1.2}Paired image-to-image generation}{854}{subsubsection.25.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {25.7.1.3}Unpaired image-to-image generation}{854}{subsubsection.25.7.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.2}Video generation}{855}{subsection.25.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.3}Audio generation}{856}{subsection.25.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.4}Text generation}{857}{subsection.25.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.5}Imitation Learning}{858}{subsection.25.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.6}Domain Adaptation}{858}{subsection.25.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {25.7.7}Design, Art and Creativity}{859}{subsection.25.7.7}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{V\hspace {1em}Discovery}{861}{part.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {26}Discovery methods: an overview}{863}{chapter.26}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {26.1}Introduction}{863}{section.26.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {26.2}Overview of \cref {part:discovery}}{864}{section.26.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {27}Latent factor models}{865}{chapter.27}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.1}Introduction}{865}{section.27.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.2}Mixture models}{865}{section.27.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.2.1}Gaussian mixture models (GMMs)}{866}{subsection.27.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.2.2}Bernoulli mixture models}{868}{subsection.27.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.2.3}Gaussian scale mixtures}{868}{subsection.27.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.3.1}Student $t$ distribution as GSM}{869}{subsubsection.27.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.3.2}Laplace distribution as GSM}{869}{subsubsection.27.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.3.3}Spike and slab distribution}{869}{subsubsection.27.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.3.4}Horseshoe distribution}{869}{subsubsection.27.2.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.2.4}Using GMMs as a prior for inverse imaging problems}{870}{subsection.27.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.4.1}Why does the method work?}{872}{subsubsection.27.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.4.2}Speeding up inference using discriminative models}{873}{subsubsection.27.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.2.4.3}Blind inverse problems}{873}{subsubsection.27.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.3}Factor analysis}{873}{section.27.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.1}Vanilla factor analysis}{873}{subsection.27.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.1.1}FA as a Gaussian with low-rank plus diagonal covariance}{874}{subsubsection.27.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.1.2}Computing the posterior}{874}{subsubsection.27.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.1.3}Computing the likelihood}{875}{subsubsection.27.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.1.4}Unidentifiability of the parameters}{876}{subsubsection.27.3.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.2}Probabilistic PCA}{877}{subsection.27.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.2.1}Connection to PCA}{877}{subsubsection.27.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.2.2}Computing the posterior}{878}{subsubsection.27.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.2.3}Computing the likelihood}{879}{subsubsection.27.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.2.4}EM for (P)PCA}{879}{subsubsection.27.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.2.5}EM vs eigenvector methods}{880}{subsubsection.27.3.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.3}Factor analysis models for paired data}{880}{subsection.27.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.3.1}Supervised PCA}{880}{subsubsection.27.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.3.2}Partial least squares}{881}{subsubsection.27.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.3.3}Canonical correlation analysis}{882}{subsubsection.27.3.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.4}Factor analysis with exponential family likelihoods}{883}{subsection.27.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.4.1}Example: binary PCA}{884}{subsubsection.27.3.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.3.4.2}Example: categorical PCA}{884}{subsubsection.27.3.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.5}Factor analysis with DNN likelihoods}{884}{subsection.27.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.3.6}Factor analysis with GP likelihoods (GP-LVM)}{885}{subsection.27.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.4}Mixture of factor analysers}{887}{section.27.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.4.1}Model definition}{887}{subsection.27.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.4.2}Model fitting}{888}{subsection.27.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.4.2.1}Model fitting using EM}{888}{subsubsection.27.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.4.2.2}Model fitting using SGD}{889}{subsubsection.27.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.4.2.3}Model selection}{889}{subsubsection.27.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.4.3}MixFA for image generation}{889}{subsection.27.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.5}LFMs with non-Gaussian priors}{893}{section.27.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.5.1}Non-negative matrix factorization (NMF)}{893}{subsection.27.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.5.2}Multinomial PCA}{894}{subsection.27.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.5.2.1}Example: roll call data}{894}{subsubsection.27.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.5.2.2}Advantage of Dirichlet prior over Gaussian prior}{895}{subsubsection.27.5.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.5.2.3}Connection to mixture models}{896}{subsubsection.27.5.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.6}Topic models}{897}{section.27.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.6.1}Latent Dirichlet Allocation (LDA)}{897}{subsection.27.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.6.1.1}Model definition}{897}{subsubsection.27.6.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.6.1.2}Polysemy}{899}{subsubsection.27.6.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.6.1.3}Posterior inference}{900}{subsubsection.27.6.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.6.1.4}Determining the number of topics}{900}{subsubsection.27.6.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.6.2}Correlated topic model}{900}{subsection.27.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.6.3}Dynamic topic model}{901}{subsection.27.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.6.4}LDA-HMM}{902}{subsection.27.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {27.7}Independent components analysis (ICA)}{904}{section.27.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.1}Noiseless ICA model}{905}{subsection.27.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.2}The need for non-Gaussian priors}{906}{subsection.27.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.3}Maximum likelihood estimation}{908}{subsection.27.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.4}Alternatives to MLE}{908}{subsection.27.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.7.4.1}Maximizing non-Gaussianity}{909}{subsubsection.27.7.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.7.4.2}Minimizing total correlation}{909}{subsubsection.27.7.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {27.7.4.3}Maximizing mutual information (InfoMax)}{909}{subsubsection.27.7.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.5}Sparse coding}{910}{subsection.27.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {27.7.6}Nonlinear ICA}{911}{subsection.27.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {28}State-space models}{913}{chapter.28}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {28.1}Introduction}{913}{section.28.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.1.1}Models}{913}{subsection.28.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.1.2}Inferential goals}{914}{subsection.28.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {28.2}Hidden Markov models (HMMs)}{915}{section.28.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.2.1}Parameterization}{915}{subsection.28.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {28.2.1.1}Discrete likelihoods}{915}{subsubsection.28.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {28.2.1.2}Gaussian likelihoods}{916}{subsubsection.28.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.2.2}Application: time series segmentation}{917}{subsection.28.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.2.3}Parameter estimation}{919}{subsection.28.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.2.4}Model extensions}{919}{subsection.28.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {28.3}Linear dynamical systems}{919}{section.28.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.3.1}Example: 2d tracking problem}{920}{subsection.28.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.3.2}Example: Recursive least squares}{922}{subsection.28.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {28.4}Non-linear dynamical systems}{925}{section.28.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.4.1}Example: stochastic volatility models}{925}{subsection.28.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.4.2}Example: nonlinear 2d tracking problem}{925}{subsection.28.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.4.3}Simultaneous localization and mapping (SLAM)}{926}{subsection.28.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.4.4}Switching linear dynamical systems}{928}{subsection.28.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.4.5}Deep SSMs}{930}{subsection.28.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {28.5}Time series forecasting}{930}{section.28.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.5.1}Structural time series models}{930}{subsection.28.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.5.2}Prophet}{931}{subsection.28.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.5.3}Gaussian processes for timeseries forecasting}{931}{subsection.28.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {28.5.3.1}Example: Mauna Loa revisited}{932}{subsubsection.28.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {28.5.4}Neural forecasting methods}{933}{subsection.28.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {29}Graph learning}{935}{chapter.29}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.1}Introduction}{935}{section.29.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.2}Latent variable models for graphs}{935}{section.29.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.2.1}Stochastic block model}{935}{subsection.29.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.2.2}Mixed membership stochastic block model}{937}{subsection.29.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.2.3}Infinite relational model}{939}{subsection.29.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.2.3.1}Learning ontologies}{939}{subsubsection.29.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.2.3.2}Clustering based on relations and features}{940}{subsubsection.29.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.3}Graphical model structure learning}{941}{section.29.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.3.1}Applications}{941}{subsection.29.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.3.2}Relevance networks}{943}{subsection.29.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.4}Learning tree structures}{944}{section.29.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.4.1}Directed or undirected tree?}{944}{subsection.29.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.4.2}Chow-Liu algorithm}{945}{subsection.29.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.4.3}Finding the MAP forest}{946}{subsection.29.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.4.4}Mixtures of trees}{947}{subsection.29.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.5}Learning DAG structures}{947}{section.29.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.1}Faithfulness}{948}{subsection.29.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.2}Markov equivalence}{948}{subsection.29.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.3}Bayesian model selection: statistical foundations}{949}{subsection.29.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.3.1}Deriving the likelihood}{950}{subsubsection.29.5.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.3.2}Deriving the marginal likelihood}{950}{subsubsection.29.5.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.3.3}Setting the prior}{951}{subsubsection.29.5.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.3.4}Example: analysis of the college plans dataset}{952}{subsubsection.29.5.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.3.5}Marginal likelihood for non-tabular CPDs}{952}{subsubsection.29.5.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.4}Bayesian model selection: algorithms}{953}{subsection.29.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.4.1}The K2 algorithm for known node orderings}{953}{subsubsection.29.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.4.2}Dynamic programming algorithms}{953}{subsubsection.29.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.4.3}Scaling up to larger graphs}{953}{subsubsection.29.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.4.4}Hill climbing methods for approximating the mode}{954}{subsubsection.29.5.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.4.5}Sampling methods}{955}{subsubsection.29.5.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.5}Constraint-based approach}{955}{subsection.29.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.5.1}IC algorithm}{955}{subsubsection.29.5.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.5.2}PC algorithm}{956}{subsubsection.29.5.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.5.3}Frequentist vs Bayesian methods}{957}{subsubsection.29.5.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.6}Methods based on sparse optimization}{957}{subsection.29.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.7}Consistent estimators}{958}{subsection.29.5.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.5.8}Handling latent variables}{958}{subsection.29.5.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.1}Approximating the marginal likelihood}{959}{subsubsection.29.5.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.2}BIC approximation}{959}{subsubsection.29.5.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.3}Cheeseman-Stutz approximation}{959}{subsubsection.29.5.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.4}Variational Bayes EM}{960}{subsubsection.29.5.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.5}Example: college plans revisited}{960}{subsubsection.29.5.8.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.6}Structural EM}{961}{subsubsection.29.5.8.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.7}Discovering hidden variables}{962}{subsubsection.29.5.8.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.8}Example: Google's Rephil}{964}{subsubsection.29.5.8.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.9}Spectral methods}{966}{subsubsection.29.5.8.9}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.5.8.10}Constraint-based methods for learning ADMGs}{966}{subsubsection.29.5.8.10}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.6}Learning undirected graph structures}{966}{section.29.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.6.1}Dependency networks}{967}{subsection.29.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.6.2}Graphical lasso for GGMs}{968}{subsection.29.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.6.2.1}MLE for a GGM}{968}{subsubsection.29.6.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.6.2.2}Promoting sparsity}{969}{subsubsection.29.6.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.6.3}Graphical lasso for discrete MRFs/CRFs}{970}{subsection.29.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.6.4}Bayesian inference for undirected graph structures }{971}{subsection.29.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {29.7}Learning causal DAGs}{972}{section.29.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.7.1}Learning cause-effect pairs}{973}{subsection.29.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.7.1.1}Algorithmic information theory}{973}{subsubsection.29.7.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.7.1.2}Additive noise models}{974}{subsubsection.29.7.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.7.1.3}Nonlinear additive noise models}{974}{subsubsection.29.7.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.7.1.4}Linear models with non-Gaussian noise}{975}{subsubsection.29.7.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {29.7.1.5}Information-geometric causal inference}{975}{subsubsection.29.7.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.7.2}Learning causal DAGs from interventional data}{976}{subsection.29.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {29.7.3}Learning from low-level inputs}{977}{subsection.29.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {30}Non-parametric Bayesian models}{979}{chapter.30}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.1}Introduction}{979}{section.30.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.2}Dirichlet processes}{980}{section.30.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.2.1}Definition of a DP}{980}{subsection.30.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.2.2}Stick breaking construction of the DP}{982}{subsection.30.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.2.3}The Chinese restaurant process (CRP)}{983}{subsection.30.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.2.4}Dirichlet process mixture models}{985}{subsection.30.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {30.2.4.1}Fitting a DP mixture model}{987}{subsubsection.30.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.3}Generalizations of the Dirichlet process}{990}{section.30.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.3.1}Pitman-Yor process}{991}{subsection.30.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.3.2}Dependent random probability measures}{992}{subsection.30.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.4}The Indian buffet process and the Beta process}{994}{section.30.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.5}Small-variance asymptotics}{997}{section.30.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.6}Completely random measures}{1000}{section.30.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.7}L\'evy processes}{1001}{section.30.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {30.8}Point processes with repulsion and reinforcement}{1003}{section.30.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.8.1}Poisson process}{1003}{subsection.30.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.8.2}Renewal process}{1004}{subsection.30.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.8.3}Hawkes process}{1005}{subsection.30.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.8.4}Gibbs point process}{1007}{subsection.30.8.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {30.8.5}Determinantal point process}{1008}{subsection.30.8.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {31}Representation learning {\bf (Unfinished)} }{1011}{chapter.31}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {31.1}CLIP}{1011}{section.31.1}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {32}Interpretability}{1013}{chapter.32}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {32.1}Introduction}{1013}{section.32.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.1.1}The Role of Interpretability}{1014}{subsection.32.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.1.2}Terminology and Framework }{1015}{subsection.32.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {32.2}Methods for Interpretable Machine Learning}{1019}{section.32.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.2.1}Inherently Interpretable Models: The Model is its Explanation }{1019}{subsection.32.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.2.2}Semi-Inherently Interpretable Models: Example-Based Methods}{1021}{subsection.32.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.2.3}Post-hoc or Joint training: The Explanation gives a Partial View of the Model}{1022}{subsection.32.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {32.2.3.1}What does the explanation consist of?}{1022}{subsubsection.32.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {32.2.3.2}How the explanation is computed}{1024}{subsubsection.32.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.2.4}Transparency and Visualization}{1026}{subsection.32.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {32.3}Properties: The Abstraction Between Context and Method}{1027}{section.32.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.3.1}Properties of Explanations from Interpretable Machine Learning}{1028}{subsection.32.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.3.2}Properties of Explanations from Cognitive Science }{1030}{subsection.32.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {32.4}Evaluation of Interpretable Machine Learning Models }{1031}{section.32.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.4.1}Computational Evaluation: Does the Method have Desired Properties?}{1032}{subsection.32.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {32.4.2}User Study-based Evaluation: Does the Method Help a User Perform a Task?}{1037}{subsection.32.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {32.4.2.1}User Studies in Real Contexts.}{1037}{subsubsection.32.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {32.4.2.2}Basic elements of user studies}{1037}{subsubsection.32.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {32.4.2.3}User Studies in Synthetic Contexts}{1040}{subsubsection.32.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {32.5}Discussion: How to Think about Interpretable Machine Learning}{1040}{section.32.5}%
\defcounter {refsection}{0}\relax 
\contentsline {part}{VI\hspace {1em}Decision making}{1047}{part.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {33}Multi-step decision problems}{1049}{chapter.33}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.1}Introduction}{1049}{section.33.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.2}Decision (influence) diagrams}{1049}{section.33.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.2.1}Example: oil wildcatter}{1049}{subsection.33.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.2.2}Information arcs}{1050}{subsection.33.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.2.3}Value of information}{1051}{subsection.33.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.2.4}Computing the optimal policy}{1052}{subsection.33.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.3}A/B testing}{1052}{section.33.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.3.1}A Bayesian approach}{1053}{subsection.33.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.3.1.1}Optimal policy}{1053}{subsubsection.33.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.3.1.2}Optimal sample size}{1054}{subsubsection.33.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.3.1.3}Regret}{1055}{subsubsection.33.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.3.1.4}Expected error rate}{1055}{subsubsection.33.3.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.3.2}Example}{1056}{subsection.33.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.4}Contextual bandits}{1057}{section.33.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.1}Types of bandit}{1057}{subsection.33.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.2}Applications}{1059}{subsection.33.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.3}Exploration-exploitation tradeoff}{1059}{subsection.33.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.4}The optimal solution}{1059}{subsection.33.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.5}Upper confidence bounds (UCB)}{1061}{subsection.33.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.4.5.1}Frequentist approach}{1062}{subsubsection.33.4.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.4.5.2}Bayesian approach}{1062}{subsubsection.33.4.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.4.5.3}Example}{1063}{subsubsection.33.4.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.6}Thompson sampling}{1063}{subsection.33.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.4.7}Regret}{1064}{subsection.33.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.5}Markov decision problems}{1065}{section.33.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.5.1}Basics}{1066}{subsection.33.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.5.2}Partially observed MDPs}{1067}{subsection.33.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.5.3}Episodes and returns}{1068}{subsection.33.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.5.4}Value functions}{1068}{subsection.33.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.5.5}Optimal value functions and policies}{1069}{subsection.33.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {33.5.5.1}Example}{1070}{subsubsection.33.5.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {33.6}Planning in an MDP}{1070}{section.33.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.6.1}Value iteration}{1071}{subsection.33.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.6.2}Policy iteration}{1072}{subsection.33.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {33.6.3}Linear programming}{1073}{subsection.33.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {34}Reinforcement learning}{1075}{chapter.34}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.1}Introduction}{1075}{section.34.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.1.1}Overview of methods}{1075}{subsection.34.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.1.2}Value based methods}{1077}{subsection.34.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.1.3}Policy search methods}{1077}{subsection.34.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.1.4}Model-based RL}{1077}{subsection.34.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.1.5}Exploration-exploitation tradeoff}{1078}{subsection.34.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.1.5.1}$\epsilon $-greedy}{1078}{subsubsection.34.1.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.1.5.2}Boltzmann exploration}{1078}{subsubsection.34.1.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.1.5.3}Upper confidence bounds and Thompson sampling}{1078}{subsubsection.34.1.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.1.5.4}Optimal solution using Bayes-adaptive MDPs}{1079}{subsubsection.34.1.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.2}Value-based RL}{1080}{section.34.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.1}Monte Carlo RL}{1080}{subsection.34.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.2}Temporal difference (TD) learning}{1080}{subsection.34.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.3}TD learning with eligibility traces}{1081}{subsection.34.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.4}SARSA: on-policy TD control}{1082}{subsection.34.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.5}Q-learning: off-policy TD control}{1083}{subsection.34.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.2.5.1}Example}{1083}{subsubsection.34.2.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.2.5.2}Double Q-learning}{1083}{subsubsection.34.2.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.2.6}Deep Q-network (DQN)}{1085}{subsection.34.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.3}Policy-based RL}{1086}{section.34.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.1}The policy gradient theorem}{1086}{subsection.34.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.2}REINFORCE}{1087}{subsection.34.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.3}Actor-critic methods}{1088}{subsection.34.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.3.3.1}A2C and A3C}{1089}{subsubsection.34.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.3.3.2}Eligibility traces}{1089}{subsubsection.34.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.4}Bound optimization methods}{1090}{subsection.34.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.5}Deterministic policy gradient methods}{1092}{subsection.34.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.3.6}Gradient-free methods}{1093}{subsection.34.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.4}Model-based RL}{1093}{section.34.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.1}Model predictive control (MPC)}{1093}{subsection.34.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.1.1}Heuristic search}{1094}{subsubsection.34.4.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.1.2}Monte-Carlo tree search (MCTS)}{1094}{subsubsection.34.4.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.1.3}Trajectory optimization for continuous actions}{1095}{subsubsection.34.4.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.2}Combining model-based and model-free}{1095}{subsection.34.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.3}MBRL using Gaussian processes}{1095}{subsection.34.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.3.1}PILCO}{1095}{subsubsection.34.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.3.2}GP-MPC}{1097}{subsubsection.34.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.4}MBRL using DNNs}{1097}{subsection.34.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.5}MBRL using latent-variable models}{1097}{subsection.34.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.5.1}World models}{1097}{subsubsection.34.4.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.4.5.2}PlaNet and Dreamer}{1099}{subsubsection.34.4.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.4.6}Robustness to model errors}{1100}{subsection.34.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.5}Off-policy learning}{1100}{section.34.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.5.1}Basic techniques}{1101}{subsection.34.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.5.1.1}Direct method}{1101}{subsubsection.34.5.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.5.1.2}Importance sampling}{1101}{subsubsection.34.5.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.5.1.3}Doubly robust}{1102}{subsubsection.34.5.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.5.1.4}Behavior regularized method}{1103}{subsubsection.34.5.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.5.2}The curse of horizon}{1104}{subsection.34.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.5.3}The deadly triad}{1105}{subsection.34.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {34.6}Control as inference}{1106}{section.34.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.6.1}Maximum entropy reinforcement learning}{1106}{subsection.34.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.6.2}Other approaches}{1109}{subsection.34.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {34.6.3}Imitation learning}{1110}{subsection.34.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.6.3.1}Imitation learning by behavior cloning}{1110}{subsubsection.34.6.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.6.3.2}Imitation learning by inverse reinforcement learning}{1111}{subsubsection.34.6.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {34.6.3.3}Imitation learning by divergence minimization}{1111}{subsubsection.34.6.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {35}Causality}{1113}{chapter.35}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.1}Introduction}{1113}{section.35.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.1.1}Why is causality different than other forms of ML?}{1113}{subsection.35.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.2}Causal Formalism}{1115}{section.35.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.2.1}Structural Causal Models}{1115}{subsection.35.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.2.2}Causal DAGs}{1117}{subsection.35.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.2.3}Identification}{1119}{subsection.35.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.2.4}Counterfactuals and the Causal Hierarchy}{1120}{subsection.35.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.3}Randomized Control Trials}{1122}{section.35.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.4}Confounder Adjustment}{1123}{section.35.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.1}Causal Estimand, Statistical Estimand, and Identification}{1123}{subsection.35.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Overlap}{1126}{section*.640}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.2}ATE Estimation with Observed Confounders}{1126}{subsection.35.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.2.1}Outcome Model Adjustment}{1126}{subsubsection.35.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Linear regression}{1127}{section*.641}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.2.2}Propensity Score Adjustment}{1127}{subsubsection.35.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.2.3}Double Machine Learning}{1129}{subsubsection.35.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.2.4}Cross Fitting}{1131}{subsubsection.35.4.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.3}Uncertainity Quantification}{1131}{subsection.35.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.4}Matching}{1131}{subsection.35.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.5}Practical Considerations and Procedures}{1132}{subsection.35.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.5.1}What to adjust for}{1133}{subsubsection.35.4.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.5.2}Overlap}{1134}{subsubsection.35.4.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.4.5.3}Choice of Estimand and Average Treatment Effect on the Treated}{1134}{subsubsection.35.4.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.4.6}Summary and Practical Advice}{1135}{subsection.35.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.5}Instrumental Variable Strategies}{1137}{section.35.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.5.1}Additive Unobserved Confounding}{1139}{subsection.35.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Estimation}{1140}{section*.644}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.5.2}Instrument Monotonicity and Local Average Treatment Effect}{1140}{subsection.35.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.5.2.1}Estimation}{1142}{subsubsection.35.5.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.5.3}Two Stage Least Squares}{1144}{subsection.35.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.6}Difference in Differences}{1144}{section.35.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.6.1}Estimation}{1148}{subsection.35.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.7}Credibility Checks}{1148}{section.35.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.7.1}Placebo Checks}{1149}{subsection.35.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.7.2}Sensitivity Analysis to Unobserved Confounding}{1149}{subsection.35.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Austen plots}{1150}{figure.caption.647}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Setup}{1152}{section*.648}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Sensitivity Model}{1152}{section*.649}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Bias}{1153}{section*.650}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Sensitivity Parameters}{1154}{section*.651}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Estimating bias}{1155}{section*.652}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.7.2.1}Calibration using observed data}{1155}{subsubsection.35.7.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Grouping covariates}{1155}{section*.653}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {35.7.2.2}Practical Use}{1155}{subsubsection.35.7.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Calibration using observed data}{1156}{section*.654}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.8}The Do Calculus}{1157}{section.35.8}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.8.1}The three rules}{1157}{subsection.35.8.1}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Rule 1}{1157}{section*.655}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Rule 2}{1157}{section*.656}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Rule 3}{1158}{section*.657}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.8.2}Revisiting Backdoor Adjustment}{1158}{subsection.35.8.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {35.8.3}Frontdoor Adjustment}{1159}{subsection.35.8.3}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{Estimation}{1160}{section*.673}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {35.9}Further Reading}{1161}{section.35.9}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{Bibliography}{1175}{chapter*.674}%
