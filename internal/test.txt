\contentsline {figure}{\numberline {8.1}{\ignorespaces A state-space model represented as a graphical model. ${\bm {z}}_t$ are the hidden variables at time $t$, ${\bm {y}}_t$ are the observations (outputs), and ${\bm {u}}_t$ are the optional inputs. \relax }}{332}{figure.caption.178}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.2}{\ignorespaces The main kinds of inference for state-space models. The shaded region is the interval for which we have data. The arrow represents the time step at which we want to perform inference. $t$ is the current time, $T$ is the sequence length, $\ell $ is the lag and $h$ is the prediction horizon. \relax }}{332}{figure.caption.179}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.3}{\ignorespaces The state transition matrix $\mathbf {A}$ and observation matrix $\mathbf {B}$ for the casino HMM. Adapted from \citep [p54]{Durbin98}. \relax }}{333}{figure.caption.180}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.4}{\ignorespaces Inference in the dishonest casino. Vertical gray bars denote times when the hidden state corresponded to the loaded die. Blue lines represent the posterior probability of being in that state given different subsets of observed data. If we recover the true state exactly, the blue curve will transition at the same time as the gray bars. (a) Filtered estimate. (b) Smoothed estimates. (c) MAP trajectory. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{casino\_hmm.ipynb}. \relax }}{334}{figure.caption.181}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.5}{\ignorespaces Computing the two-slice joint distribution for an HMM from the forwards messages, backwards messages, and local evidence messages. \relax }}{339}{figure.caption.182}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.6}{\ignorespaces The trellis of states vs time for a Markov chain. Adapted from \citep {Rabiner89}. \relax }}{341}{figure.caption.183}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.7}{\ignorespaces Illustration of Viterbi decoding in a simple HMM for speech recognition. (a) A 3-state HMM for a single phone. We are visualizing the state transition diagram. We assume the observations have been vector quantized into 7 possible symbols, $C_1,\ldots ,C_7$. Each state $s_1,s_2,s_3$ has a different distribution over these symbols. Adapted from Figure 15.20 of \citep {Russell02}. (b) Illustration of the Viterbi algorithm applied to this model, with data sequence $C1, C3, C4, C6$. The columns represent time, and the rows represent states. The numbers inside the circles represent the $\delta _t(j)$ value for that state. An arrow from state $i$ at $t-1$ to state $j$ at $t$ is annotated with two numbers: the first is the probability of the $i \rightarrow j$ transition, and the second is the probability of generating observation ${\bm {y}}_{t}$ from state $j$. The red lines/ circles represent the most probable sequence of states. Adapted from Figure 24.27 of \citep {Russell95b}. \relax }}{342}{figure.caption.184}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.8}{\ignorespaces Illustration of Kalman filtering and smoothing for a linear dynamical system. (a) Observations (green cirles) are generated by an object moving to the right (true location denoted by blue squares). (b) Results of online Kalman filtering. Red cross is the posterior mean, circles are 95\% confidence ellipses derived from the posterior covariance. (c) Same as (b), but using offline Kalman smoothing. The MSE in the trajectory for filtering is 3.13, and for smoothing is 1.71. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{kf\_tracking.ipynb}. \relax }}{345}{figure.caption.185}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.9}{\ignorespaces (a) A dynamic generalization of linear regression. (b) Illustration of the recursive least squares algorithm applied to the model $p(y|x,{\bm {w}}) = \mathcal {N}(y|w_0 + w_1 x, \sigma ^2)$. We plot the marginal posterior of $w_0$ and $w_1$ vs number of data points. (Error bars represent $\mathbb {E}\left [{w_j|{\bm {y}}_{1:t},{\bm {x}}_{1:t}}\right ] \pm \sqrt {\mathbb {V}\left [ {w_j|{\bm {y}}_{1:t},{\bm {x}}_{1:t}}\right ]}$.) After seeing all the data, we converge to the offline (batch) Bayes solution, represented by the horizontal lines. (Shading represents the marginal posterior variance.) Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{kf\_linreg.ipynb}. \relax }}{346}{figure.caption.186}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.10}{\ignorespaces (a) Observations and true and estimated state. (b) Marginal distributions for time step $t=20$. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{discretized\_ssm\_student.ipynb}. \relax }}{353}{figure.caption.187}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.11}{\ignorespaces Discretized posterior of the latent state at each time step. Red cross is the true latent state. Red circle is observation. (a) Filtering. (b) Smoothing. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{discretized\_ssm\_student.ipynb}. \relax }}{353}{figure.caption.188}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.12}{\ignorespaces Nonlinear transformation of a Gaussian random variable. The prior $p(x)$ is shown on the bottom right. The function $y=g(x)$ is shown on the top right. The transformed distribution $p(y)$ is shown in the top left. A linear function induces a Gaussian distribution, but a non-linear function induces a complex distribution. (a) The solid line is the best Gaussian approximation to this. The dotted line is the EKF approximation to this. From Figure 3.4 of \citep {Thrun06}. (b) The dotted line is the UKF approximation to this. From Figure 3.7 of \citep {Thrun06}. Used with kind permission of Sebastian Thrun. \relax }}{355}{figure.caption.189}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.13}{\ignorespaces Illustration of the fact that a broad prior (a) may result in a more complex posterior than a narrow prior (b). Consequently, the EKF approximation may work poorly in situations of high uncertainty. From Figure 3.5 of \citep {Thrun06}. Used with kind permission of Dieter Fox. \relax }}{358}{figure.caption.190}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.14}{\ignorespaces Illustration of the fact that if the function function is very nonlinear (a) at the current operating point, the posterior will be less well approximated by the EKF than if the function is locally linear (b). From Figure 3.6 of \citep {Thrun06}. Used with kind permission of Dieter Fox. \relax }}{359}{figure.caption.191}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.15}{\ignorespaces An example of the unscented transform in two dimensions. From \citep {Wan01unscented}. Used with kind permission of Eric Wan. \relax }}{360}{figure.caption.192}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.16}{\ignorespaces Illustration of UKF applied to a 2d nonlinear dynamical system. (a) True underlying state and observed data. (b) UKF estimate. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{ekf\_vs\_ukf.ipynb}. \relax }}{363}{figure.caption.193}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.17}{\ignorespaces Illustration of the predict-update-project cycle of assumed density filtering. \relax }}{368}{figure.caption.194}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.18}{\ignorespaces ADF for a switching linear dynamical system with 2 discrete states. (a) GPB2 method. (b) IMM method. \relax }}{369}{figure.caption.195}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.19}{\ignorespaces A dynamic logistic regression model. ${\bm {w}}_t$ are the regression weights at time $t$, and $\eta _t = {\bm {w}}_t^{\mkern -1.5mu\mathsf {T}}{\bm {x}}_t$. Compare to \cref {fig:RLSDGM}. \relax }}{370}{figure.caption.196}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.20}{\ignorespaces Bayesian inference applied to a 2d binary logistic regression problem, $p(y=1|{\bm {x}}) = \sigma (w_0 + w_1 x_1 + w_2 x_2)$. We show the training data and the posterior predictive produced by different methods. (a) Offline MCMC approximation. (b) Offline Laplace approximation. (c) Online ADF approximation at the final step of inference. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{adf\_logistic\_regression\_demo.ipynb}. \relax }}{372}{figure.caption.197}
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {8.21}{\ignorespaces Marginal posteriors over time for the ADF method. The horizontal line is the offline MAP estimate. Generated by \href {https://github.com/probml/pyprobml/notebooks.md}{adf\_logistic\_regression\_demo.ipynb}. \relax }}{372}{figure.caption.198}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 