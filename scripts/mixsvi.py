# -*- coding: utf-8 -*-
"""mixSVI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1viTU5hK_ZRTU8byxMeVMmdJZVAp7hIOG
"""

# Commented out IPython magic to ensure Python compatibility.
# Author: Meduri Venkata Shivaditya
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
# Imports
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import tensorflow_probability as tfp
tfd = tfp.distributions

# Plot settings
# %config InlineBackend.figure_format = 'svg'

# Random seed
np.random.seed(12345)
tf.random.set_seed(12345)
#Bernoulli Mixture Model using Stochastic Variational Inference
#Traditional EM algorithm is costly especially due to the M step that focuses on maximizing 
#the likelihood and sometimes gets stuck at the local maximum
#Stochastic variational inferernce is a more efficient process to estimate the parameters
#It is extremely helpful in situations involving image data as they are generally massive

class BernoulliMixtureModel(tf.Module):
    def __init__(self, Nc, Nd):
        # Initialize
        super(BernoulliMixtureModel, self).__init__()
        self.Nc = Nc
        self.Nd = Nd
        
        # Variational distribution variables for means - Beta Distribution
        self.beta_0 = tf.Variable(tf.random.uniform((Nc, Nd), 9.9, 10.1))
        self.beta_1 = tf.Variable(tf.random.uniform((Nc, Nd), 0.9, 1.1))
        
        # Variational distribution variables for component weights
        self.counts = tf.Variable(2*tf.ones((Nc,)))

        # Prior distributions for the means
        self.mu_prior = tfd.Beta(1*tf.ones((Nc, Nd)), 1*tf.ones((Nc, Nd)))
        
        # Prior distributions for the component weights
        self.theta_prior = tfd.Dirichlet(2*tf.ones((Nc,)))
        
        
    def __call__(self, x, sampling=True, independent=True):
        """Compute losses given a batch of data.
        
        Parameters
        ----------
        x : tf.Tensor
            A batch of data
        sampling : bool
            Whether to sample from the variational posterior
            distributions (if True, the default), or just use the
            mean of the variational distributions (if False).
            
        Returns
        -------
        log_likelihoods : tf.Tensor
            Log likelihood for each sample
        kl_sum : tf.Tensor
            Sum of the KL divergences between the variational
            distributions and their priors
        """
        
        # The variational distributions
        mu = tfd.Beta(self.beta_0, self.beta_1)
        theta = tfd.Dirichlet(self.counts)
        
        # Sample from the variational distributions
        if sampling:
            Nb = x.shape[0] #number of samples in the batch
            mu_sample = mu.sample(Nb)
            theta_sample = theta.sample(Nb)
        else:
            mu_sample = tf.reshape(mu.mean(), (1, self.Nc, self.Nd))
            theta_sample = tf.reshape(theta.mean(), (1, self.Nc))
        # The mixture density
        cat=tfd.Categorical(probs=theta_sample)
        #bern = tfd.Independent(distribution=[tfd.Bernoulli(probs=mu_sample[:, i, :]) for i in range(self.Nc)], reinterpreted_batch_ndims=1)
        components=[
                tfd.Independent(distribution=tfd.Bernoulli(probs=mu_sample[:, i, :]), reinterpreted_batch_ndims=1)
                for i in range(self.Nc)]
        #print(cat)
        #bern = tfd.Independent(distribution=components, reinterpreted_batch_ndims=0)
        density = tfd.Mixture(
            cat=cat,
            components=components)     
        # Compute the mean log likelihood
        log_likelihoods = density.log_prob(x)
        
        # Compute the KL divergence sum
        mu_div    = tf.reduce_sum(tfd.kl_divergence(mu,    self.mu_prior))
        theta_div = tf.reduce_sum(tfd.kl_divergence(theta, self.theta_prior))
        kl_sum = mu_div + theta_div
        
        # Return both losses
        return log_likelihoods, kl_sum

# A BMM with 20 components in 784 dimensions
model = BernoulliMixtureModel(20, 784)

# Use the Adam optimizer
optimizer = tf.keras.optimizers.Adam(lr=1e-3)

N=1000
@tf.function
def train_step(data):
    with tf.GradientTape() as tape:
        log_likelihoods, kl_sum = model(data)
        elbo_loss = kl_sum/N - tf.reduce_mean(log_likelihoods)
    gradients = tape.gradient(elbo_loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# Make a TensorFlow Dataset from that data
def mnist_data():
    #Downloading data from tensorflow datasets
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x = (x_train>=150).astype('int') #Coverting to binary
    x_train = x[np.random.randint(x.shape[0], size=1000)]
    x_train = x_train.reshape((1000, 784))
    return x_train
data = mnist_data()
batch_size = 500
dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)

# Fit the model
EPOCHS = 10
for epoch in range(EPOCHS):
  print('.', end='')
  for d in dataset:    
    train_step(d)

beta = tfd.Beta(model.beta_0, model.beta_1)
mns = beta.mean().numpy()
print(model.beta_1.numpy().mean())

print(mns)

mns[mns>0.5] = 1
mns[mns<=0.5] = 0

def plot_data(mns):
    fig, ax = plt.subplots(4, 5)
    k = 0
    for i in range(4):
        for j in range(5):
            ax[i][j].imshow(mns[k].reshape(28, 28), cmap=plt.cm.gray)
            #ax[i][j].set_title("%1.2f" % wts[k])
            ax[i][j].axis("off")
            k = k + 1
    fig.tight_layout(pad=1.0)
    plt.savefig("mixBernoulliMnist.png", dpi=300)
    plt.show()

import numpy as np
plot_data(mns)

