{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process binary classification in 1d\n",
    "# Code is based on\n",
    "# https://github.com/aloctavodia/BAP/blob/master/code/Chp7/07_Gaussian%20process.ipynb\n",
    "\n",
    "\n",
    "try:\n",
    "    import pymc3 as pm\n",
    "except:\n",
    "    %pip install pymc3\n",
    "    import pymc3 as pm\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.special import expit as logistic\n",
    "\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import arviz as az\n",
    "except:\n",
    "    %pip install arviz\n",
    "    import arviz as az\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import load_iris\n",
    "except:\n",
    "    %pip install scikit-learn\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df_iris = pd.DataFrame(data=iris.data, columns=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"])\n",
    "df_iris[\"species\"] = pd.Series(iris.target_names[y], dtype=\"category\")\n",
    "\n",
    "\n",
    "# Covnert to a two-class problem, and extract 1 feature to make 1d\n",
    "df = df_iris.query(\"species == ('setosa', 'versicolor')\")\n",
    "y = pd.Categorical(df[\"species\"]).codes\n",
    "x_1 = df[\"sepal_length\"].values\n",
    "X_1 = x_1[:, None]\n",
    "\n",
    "# For evaluating posterior predictive\n",
    "X_new = np.linspace(np.floor(x_1.min()), np.ceil(x_1.max()), 200)[:, None]\n",
    "\n",
    "\n",
    "def find_midpoint(array1, array2, value):\n",
    "    array1 = np.asarray(array1)\n",
    "    idx0 = np.argsort(np.abs(array1 - value))[0]\n",
    "    idx1 = idx0 - 1 if array1[idx0] > value else idx0 + 1\n",
    "    if idx1 == len(array1):\n",
    "        idx1 -= 1\n",
    "    return (array2[idx0] + array2[idx1]) / 2\n",
    "\n",
    "\n",
    "if 1:\n",
    "    # Posterior over length scale l of kernel\n",
    "    with pm.Model() as model_iris:\n",
    "        # ℓ = pm.HalfCauchy(\"ℓ\", 1)\n",
    "        ℓ = pm.Gamma(\"ℓ\", 2, 0.5)\n",
    "        cov = pm.gp.cov.ExpQuad(1, ℓ)\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X_1)\n",
    "        # logistic inverse link function and Bernoulli likelihood\n",
    "        y_ = pm.Bernoulli(\"y\", p=pm.math.sigmoid(f), observed=y)\n",
    "        trace_iris = pm.sample(1000, chains=1, cores=1, compute_convergence_checks=False)\n",
    "\n",
    "    # Posterior predictive\n",
    "\n",
    "    with model_iris:\n",
    "        f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "        pred_samples = pm.sample_posterior_predictive(trace_iris, var_names=[f_pred], samples=1000)\n",
    "\n",
    "    # Plot results\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    fp = logistic(pred_samples[\"f_pred\"])\n",
    "    fp_mean = np.mean(fp, 0)\n",
    "\n",
    "    ax.plot(X_new[:, 0], fp_mean)\n",
    "    # plot the data (with some jitter) and the true latent function\n",
    "    ax.scatter(x_1, np.random.normal(y, 0.02), marker=\".\", color=[f\"C{x}\" for x in y])\n",
    "\n",
    "    az.plot_hdi(X_new[:, 0], fp, color=\"C2\")\n",
    "\n",
    "    db = np.array([find_midpoint(f, X_new[:, 0], 0.5) for f in fp])\n",
    "    db_mean = db.mean()\n",
    "    db_hpd = az.hdi(db)\n",
    "    ax.vlines(db_mean, 0, 1, color=\"k\")\n",
    "    ax.fill_betweenx([0, 1], db_hpd[0], db_hpd[1], color=\"k\", alpha=0.5)\n",
    "    ax.set_xlabel(\"sepal_length\")\n",
    "    ax.set_ylabel(\"θ\", rotation=0)\n",
    "    pml.savefig(\"gp_classify_iris1.pdf\", dpi=300)\n",
    "\n",
    "# Change kernel to be sum of SE and linear, to improve tail behavior\n",
    "\n",
    "with pm.Model() as model_iris2:\n",
    "    # ℓ = pm.HalfCauchy(\"ℓ\", 1)\n",
    "    ℓ = pm.Gamma(\"ℓ\", 2, 0.5)\n",
    "    c = pm.Normal(\"c\", x_1.min())\n",
    "    τ = pm.HalfNormal(\"τ\", 5)\n",
    "    cov = pm.gp.cov.ExpQuad(1, ℓ) + τ * pm.gp.cov.Linear(1, c) + pm.gp.cov.WhiteNoise(1e-5)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    f = gp.prior(\"f\", X=X_1)\n",
    "    # logistic inverse link function and Bernoulli likelihood\n",
    "    y_ = pm.Bernoulli(\"y\", p=pm.math.sigmoid(f), observed=y)\n",
    "    trace_iris2 = pm.sample(1000, chains=1, cores=1, ompute_convergence_checks=False)\n",
    "\n",
    "with model_iris2:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "    pred_samples = pm.sample_posterior_predictive(trace_iris2, vars=[f_pred], samples=1000)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "fp = logistic(pred_samples[\"f_pred\"])\n",
    "fp_mean = np.mean(fp, 0)\n",
    "\n",
    "ax.scatter(x_1, np.random.normal(y, 0.02), marker=\".\", color=[f\"C{ci}\" for ci in y])\n",
    "\n",
    "db = np.array([find_midpoint(f, X_new[:, 0], 0.5) for f in fp])\n",
    "db_mean = db.mean()\n",
    "db_hpd = az.hdi(db)\n",
    "ax.vlines(db_mean, 0, 1, color=\"k\")\n",
    "ax.fill_betweenx([0, 1], db_hpd[0], db_hpd[1], color=\"k\", alpha=0.5)\n",
    "\n",
    "ax.plot(X_new[:, 0], fp_mean, \"C2\", lw=3)\n",
    "az.plot_hdi(X_new[:, 0], fp, color=\"C2\")\n",
    "\n",
    "ax.set_xlabel(\"sepal_length\")\n",
    "ax.set_ylabel(\"θ\", rotation=0)\n",
    "pml.savefig(\"gp_classify_iris2.pdf\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
