{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5db644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance of tree ensembles. Based on the email spam example from chapter 10 of \"Elements of statistical learning\". Code is from Andrey Gaskov's site:\n",
    "\n",
    "#https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/examples/Spam.ipynb\n",
    "\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    %pip install pandas\n",
    "    import pandas as pd\n",
    "from matplotlib import transforms, pyplot as plt\n",
    "import numpy as np\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except:\n",
    "    %pip install scikit-learn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "# omit numpy warnings (don't do it in real work)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "np.warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "\n",
    "# define plots common properties and color constants\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "ORANGE, BLUE, PURPLE = '#FF8C00', '#0000FF', '#A020F0'\n",
    "GRAY1, GRAY4, GRAY7 = '#231F20', '#646369', '#929497'\n",
    "\n",
    "\n",
    "# we will calculate train and test error rates for all models\n",
    "def error_rate(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "\"\"\"Get data\"\"\"\n",
    "\n",
    "df = pd.read_csv(\"https://github.com/empathy87/The-Elements-of-Statistical-Learning-Python-Notebooks/blob/master/data/Spam.txt?raw=True\")\n",
    "df.head()\n",
    "\n",
    "# PAGE 301. We coded spam as 1 and email as zero. A test set of size 1536 was\n",
    "#           randomly chosen, leaving 3065 observations in the training set.\n",
    "target = 'spam'\n",
    "columns = ['word_freq_make', 'word_freq_address', 'word_freq_all',\n",
    "           'word_freq_3d', 'word_freq_our', 'word_freq_over',\n",
    "           'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
    "           'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
    "           'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
    "           'word_freq_free', 'word_freq_business', 'word_freq_email',\n",
    "           'word_freq_you', 'word_freq_credit', 'word_freq_your',\n",
    "           'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
    "           'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
    "           'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
    "           'word_freq_telnet', 'word_freq_857', 'word_freq_data',\n",
    "           'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
    "           'word_freq_1999', 'word_freq_parts', 'word_freq_pm',\n",
    "           'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
    "           'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
    "           'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "           'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "           'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
    "           'capital_run_length_longest', 'capital_run_length_total']\n",
    "# let's give columns more compact names\n",
    "features = ['make', 'address', 'all', '3d', 'our', 'over', 'remove',\n",
    "            'internet', 'order', 'mail', 'receive', 'will', 'people',\n",
    "            'report', 'addresses', 'free', 'business', 'email', 'you',\n",
    "            'credit', 'your', 'font', '000', 'money', 'hp', 'hpl',\n",
    "            'george', '650', 'lab', 'labs', 'telnet', '857', 'data',\n",
    "            '415', '85', 'technology', '1999', 'parts', 'pm', 'direct',\n",
    "            'cs', 'meeting', 'original', 'project', 're', 'edu', 'table',\n",
    "            'conference', 'ch_;', 'ch(', 'ch[', 'ch!', 'ch$', 'ch#',\n",
    "            'CAPAVE', 'CAPMAX', 'CAPTOT']\n",
    "\n",
    "X, y = df[columns].values, df[target].values\n",
    "\n",
    "# split by test column value\n",
    "is_test = df.test.values\n",
    "X_train, X_test = X[is_test == 0], X[is_test == 1]\n",
    "y_train, y_test = y[is_test == 0], y[is_test == 1]\n",
    "\n",
    "\"\"\" Logistic regression\n",
    "\n",
    "As a sanity check, we try to match p301  test error rate of 7.6%.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "except:\n",
    "    %pip install statsmodels\n",
    "    import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = sm.Logit(y_train, sm.add_constant(X_train)).fit(disp=False)\n",
    "# 0.5 is a threshold\n",
    "y_test_hat = (lr_clf.predict(sm.add_constant(X_test)) > 0.5).astype(int)\n",
    "lr_error_rate = error_rate(y_test, y_test_hat)\n",
    "print(f'Logistic Regression Test Error Rate: {lr_error_rate*100:.1f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PAGE 590. A random forest classifier achieves 4.88% misclassification error\n",
    "#           on the spam test data, which compares well with all other methods,\n",
    "#           and is not significantly worse than gradient boosting at 4.5%.\n",
    "ntrees_list = [10, 50, 100, 200, 300, 400, 500]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    rf_clf = RandomForestClassifier(\n",
    "        n_estimators=ntrees,\n",
    "        random_state=10\n",
    "    ).fit(X_train, y_train)\n",
    "    y_test_hat = rf_clf.predict(X_test)\n",
    "    rf_error_rate = error_rate(y_test, y_test_hat)\n",
    "    rf_errors.append(rf_error_rate)\n",
    "    print(f'RF {ntrees} trees, test error {rf_error_rate*100:.1f}%')\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "except:\n",
    "    %pip install catboost\n",
    "    from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "boost_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    boost_clf = CatBoostClassifier(\n",
    "        iterations=ntrees,\n",
    "        random_state=10,\n",
    "        learning_rate=0.2,\n",
    "        verbose=False\n",
    "    ).fit(X_train, y_train)\n",
    "    y_test_hat = boost_clf.predict(X_test)\n",
    "    boost_error_rate = error_rate(y_test, y_test_hat)\n",
    "    boost_errors.append(boost_error_rate)\n",
    "    print(f'Boosting {ntrees} trees, test error {boost_error_rate*100:.1f}%')\n",
    "    \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag_errors = []\n",
    "for ntrees in ntrees_list:\n",
    "    bag_clf = BaggingClassifier(\n",
    "        n_estimators=ntrees,\n",
    "        random_state=10,\n",
    "        bootstrap=True\n",
    "    ).fit(X_train, y_train)\n",
    "    y_test_hat = bag_clf.predict(X_test)\n",
    "    bag_error_rate = error_rate(y_test, y_test_hat)\n",
    "    bag_errors.append(bag_error_rate)\n",
    "    print(f'Bagged {ntrees} trees, test error {bag_error_rate*100:.1f}%')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ntrees_list, rf_errors, 'o-', color='blue', label='RF')\n",
    "plt.plot(ntrees_list, boost_errors, 'x-', color='green', label='Boosting')\n",
    "plt.plot(ntrees_list, bag_errors, '^-', color='orange', label='Bagging')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Test error')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/spam_tree_ensemble_compare.pdf', dpi=300)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
