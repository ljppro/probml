{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc9c61b",
   "metadata": {},
   "source": [
    "# Logistic regression on the iris flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is based on AurÃ©lien Geron's code\n",
    "# https://github.com/ageron/handson-ml2/blob/master/04_training_linear_models.ipynb\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "from ipywidgets import interact, FloatSlider, fixed\n",
    "\n",
    "\n",
    "try:\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "except ModuleNotFoundError:\n",
    "    %pip install -qq git+https://github.com/probml/probml-utils.git\n",
    "    from probml_utils import latexify, savefig, is_latexify_enabled\n",
    "\n",
    "# Load datset\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66de571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latexify(width_scale_factor=1.1, fig_height=1.5)\n",
    "SCATTER_SIZE = 10 if is_latexify_enabled() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e5d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/home/rohit_khoiwal/.local/lib/python3.8/site-packages/matplotlib/patches.py:1450: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.verts = np.dot(coords, M) + [\n",
      "/home/rohit_khoiwal/.local/lib/python3.8/site-packages/probml_utils/plotting.py:62: UserWarning: renaming figure/iris-logreg-1d_latexified.pdf to figure/iris-logreg-1d_latexified.pdf because LATEXIFY is True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving image to figure/iris-logreg-1d_latexified.pdf\n",
      "Figure size: [5.45454545 1.5       ]\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Fig 2.11\n",
    "# Initially use 1 feature, 2 classes\n",
    "\n",
    "x = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(jnp.int32)  # 1 if Iris-Virginica, else 0'\n",
    "\n",
    "logistic_regression = LogisticRegression(solver=\"lbfgs\", C=1)\n",
    "logistic_regression.fit(x, y)\n",
    "\n",
    "x_new = jnp.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = logistic_regression.predict_proba(x_new)\n",
    "decision_boundary = x_new[y_proba[:, 1] >= 0.5][0]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x[y == 0], y[y == 0], s=SCATTER_SIZE, c=\"b\", marker=\"s\")\n",
    "plt.scatter(x[y == 1], y[y == 1], s=SCATTER_SIZE, c=\"g\", marker=\"^\")\n",
    "plt.plot(\n",
    "    [decision_boundary, decision_boundary],\n",
    "    [-1, 2],\n",
    "    color=\"black\",\n",
    "    linestyle=\"-.\",\n",
    "    label=\"Decision boundary\",\n",
    ")\n",
    "plt.plot(x_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(x_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
    "\n",
    "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc=\"b\", ec=\"b\")\n",
    "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc=\"g\", ec=\"g\")\n",
    "\n",
    "plt.xlabel(\"Petal width (cm)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.axis([0, 3, -0.02, 1.02])\n",
    "sns.despine()\n",
    "savefig(\"iris-logreg-1d_latexified.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6238e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Logistic Regression\n",
    "@jax.jit\n",
    "def loss_function(parameters, x, y, lambd):\n",
    "    z = jnp.dot(x, parameters[\"weights\"]) + parameters[\"bias\"]\n",
    "    hypothesis_x = jax.nn.sigmoid(z)\n",
    "    regularizer = (lambd * jnp.sum(jnp.sum(parameters[\"weights\"] ** 2))) / (2 * x.shape[0])\n",
    "    return (\n",
    "        -(\n",
    "            (jnp.dot(y.T, jnp.log(hypothesis_x + 1e-7)) + jnp.dot((1 - y).T, jnp.log(1 - hypothesis_x + 1e-7)))\n",
    "            / x.shape[0]\n",
    "        )\n",
    "        + regularizer\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def logistic_regression_binary(x, y, max_iter=1000, learning_rate=0.75, lambd=0.001, random_key=1):\n",
    "    parameters = {}\n",
    "    parameters[\"weights\"] = jax.random.normal(key=jax.random.PRNGKey(random_key), shape=[x.shape[1], 1])\n",
    "    parameters[\"bias\"] = jnp.zeros(1)\n",
    "\n",
    "    optimizer = optax.adam(learning_rate=learning_rate)\n",
    "    opt_state = optimizer.init(parameters)\n",
    "\n",
    "    loss_and_grad_fn = jax.value_and_grad(loss_function)\n",
    "\n",
    "    def one_step(carry, loss):\n",
    "\n",
    "        parameters = carry[\"parameters\"]\n",
    "        x, y = carry[\"x\"], carry[\"y\"]\n",
    "        opt_state = carry[\"opt_state\"]\n",
    "        lambd = carry[\"lambd\"]\n",
    "        loss, grads = loss_and_grad_fn(parameters, x, y, lambd)\n",
    "\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, parameters)\n",
    "        parameters = optax.apply_updates(parameters, updates)\n",
    "\n",
    "        carry[\"parameters\"] = parameters\n",
    "        carry[\"x\"], carry[\"y\"] = x, y\n",
    "        carry[\"opt_state\"] = opt_state\n",
    "\n",
    "        return carry, loss\n",
    "\n",
    "    losses = None\n",
    "    carry = {\"parameters\": parameters, \"x\": x, \"y\": y, \"opt_state\": opt_state, \"lambd\": lambd}\n",
    "    last_carry, losses = jax.lax.scan(one_step, carry, losses, max_iter)\n",
    "    return last_carry[\"parameters\"], losses\n",
    "\n",
    "\n",
    "def predict_prob(parameters, x):\n",
    "    z = jnp.dot(x, parameters[\"weights\"]) + parameters[\"bias\"]\n",
    "    hypothesis_x = jax.nn.sigmoid(z)\n",
    "\n",
    "    return hypothesis_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ccaffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_loss(w0, w1, bias):\n",
    "    test_parameters = {\"weights\": jnp.array([[w0], [w1]]), \"bias\": bias}\n",
    "    return loss_function(test_parameters, train_x, train_y, 0.1)\n",
    "\n",
    "\n",
    "def decision_boundary_plot(ax, weight0, weight1, parameters, train_x, train_y, fig_name=None):\n",
    "    if is_latexify_enabled():\n",
    "        ax.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], \"bs\", markersize=1.5)\n",
    "        ax.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], \"g^\", markersize=1.5)\n",
    "    else:\n",
    "        ax.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], \"bs\")\n",
    "        ax.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], \"g^\")\n",
    "\n",
    "    # Predictions on test data\n",
    "    # y_pred = logistic_regression.predict(test_x)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    bounds_test_x0 = jnp.array([2.9, 7])\n",
    "    decision_boundary = -(weight0 * bounds_test_x0 + parameters[\"bias\"][0]) / weight1\n",
    "\n",
    "    ax.plot(bounds_test_x0, decision_boundary, \"k--\", linewidth=1)\n",
    "    ax.text(3.75, 1.70, \"Not Iris-Virginica\", color=\"b\", ha=\"center\")\n",
    "    ax.text(6.5, 1.5, \"Iris-Virginica\", color=\"g\", ha=\"center\")\n",
    "    ax.set_xlabel(\"Petal length (cm)\")\n",
    "    ax.set_ylabel(\"Petal width (cm)\")\n",
    "    ax.axis([2.9, 7, 0.8, 2.7])\n",
    "    sns.despine()\n",
    "\n",
    "    if fig_name:\n",
    "        savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc564763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "saving image to figure/iris-logreg-2d-2class\n",
      "Figure size: [5.45454545 1.5       ]\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Now use 2 features, 2 classes\n",
    "\n",
    "# Train Data\n",
    "# petal length, petal width\n",
    "train_x = iris[\"data\"][:, (2, 3)]\n",
    "\n",
    "# 1 if Iris-Virginica, else 0\n",
    "train_y = (iris[\"target\"] == 2).astype(jnp.int32)\n",
    "\n",
    "print(train_x.shape)\n",
    "\n",
    "\n",
    "# Fit model\n",
    "parameters, losses = logistic_regression_binary(train_x, train_y)\n",
    "\n",
    "\n",
    "# Test data\n",
    "\n",
    "# # Generate values for petal length and petal width\n",
    "test_featvec_x0, test_featvec_x1 = jnp.meshgrid(\n",
    "    jnp.linspace(2.9, 7, 500),\n",
    "    jnp.linspace(0.8, 2.7, 200),\n",
    ")\n",
    "\n",
    "# Plot data\n",
    "if is_latexify_enabled():\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "\n",
    "# Combine new values of petal length, petal width\n",
    "test_x = jnp.c_[test_featvec_x0.ravel(), test_featvec_x1.ravel()]\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = predict_prob(parameters, test_x)\n",
    "\n",
    "# Choose probability for label Iris-Virginica\n",
    "prob_iris_virginca = y_pred_proba.reshape(test_featvec_x0.shape)\n",
    "# Contour plot\n",
    "contour = ax.contour(test_featvec_x0, test_featvec_x1, prob_iris_virginca, cmap=plt.cm.brg)\n",
    "\n",
    "# Create decision boundary\n",
    "bounds_test_x0 = jnp.array([2.9, 7])\n",
    "decision_boundary = (\n",
    "    -(parameters[\"weights\"][0][0] * bounds_test_x0 + parameters[\"bias\"][0]) / parameters[\"weights\"][1][0]\n",
    ")\n",
    "ax.clabel(contour, inline=1)\n",
    "decision_boundary_plot(\n",
    "    ax,\n",
    "    parameters[\"weights\"][0][0],\n",
    "    parameters[\"weights\"][1][0],\n",
    "    parameters,\n",
    "    train_x,\n",
    "    train_y,\n",
    "    fig_name=\"iris-logreg-2d-2class\",\n",
    ")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b46a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a0e0fbafee42cbbc935bf493a14eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=4.374151229858398, description='weight0', max=5.874151229858398, min=2â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.draw_plots(weight0, weight1, x, y, parameters, train_x, train_y)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100\n",
    "weight0 = jnp.linspace(3, 6, n)\n",
    "weight1 = jnp.linspace(7, 10, n)\n",
    "grid_losses = jnp.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        grid_losses = grid_losses.at[i, j].set(contour_loss(weight0[i], weight1[j], parameters[\"bias\"][0]))\n",
    "x, y = jnp.meshgrid(weight0, weight1)\n",
    "\n",
    "\n",
    "def draw_plots(weight0, weight1, x, y, parameters, train_x, train_y):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5), gridspec_kw={\"width_ratios\": [3, 2.3]})\n",
    "\n",
    "    decision_boundary_plot(ax[0], weight0, weight1, parameters, train_x, train_y)\n",
    "\n",
    "    # contour_poat\n",
    "    CS = ax[1].contourf(x, y, grid_losses, alpha=0.7, cmap=\"viridis\")\n",
    "    ax[1].plot(weight0, weight1, marker=\"o\", markersize=5, markeredgecolor=\"red\", markerfacecolor=\"red\")\n",
    "    plt.contour(x, y, grid_losses, levels=50, linewidths=0.5, colors=\"black\")\n",
    "    props = dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.5)\n",
    "    loss = contour_loss(weight0, weight1, parameters[\"bias\"][0])\n",
    "    txt = f\"Loss : %0.2f\" % (loss)\n",
    "    ax[1].text(0, 1.1, txt, verticalalignment=\"top\", transform=ax[1].transAxes, bbox=props, fontsize=15)\n",
    "    sns.despine()\n",
    "    plt.colorbar(CS)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "w0 = FloatSlider(\n",
    "    min=parameters[\"weights\"][0][1] - 1.5,\n",
    "    max=parameters[\"weights\"][0][1] + 1.5,\n",
    "    step=0.5,\n",
    "    value=parameters[\"weights\"][0][1],\n",
    ")\n",
    "w1 = FloatSlider(\n",
    "    min=parameters[\"weights\"][1][0] - 1.5,\n",
    "    max=parameters[\"weights\"][1][0] + 1.5,\n",
    "    step=0.5,\n",
    "    value=parameters[\"weights\"][1][0],\n",
    ")\n",
    "\n",
    "interact(\n",
    "    draw_plots,\n",
    "    weight0=w0,\n",
    "    weight1=w1,\n",
    "    x=fixed(x),\n",
    "    y=fixed(y),\n",
    "    parameters=fixed(parameters),\n",
    "    train_x=fixed(train_x),\n",
    "    train_y=fixed(train_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67567b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving image to figure/iris-logreg-2d-2class-no-probs\n",
      "Figure size: [3.   1.25]\n"
     ]
    }
   ],
   "source": [
    "if is_latexify_enabled():\n",
    "    latexify(width_scale_factor=2, fig_height=1.25, font_size=8)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    decision_boundary_plot(\n",
    "        ax,\n",
    "        parameters[\"weights\"][0][0],\n",
    "        parameters[\"weights\"][1][0],\n",
    "        parameters,\n",
    "        train_x,\n",
    "        train_y,\n",
    "        fig_name=\"iris-logreg-2d-2class-no-probs\",\n",
    "    )\n",
    "    latexify(width_scale_factor=1.1, fig_height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cbbdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# Now use 2 features and all 3 classes\n",
    "\n",
    "# petal length, petal width\n",
    "train_x = iris[\"data\"][:, (2, 3)]\n",
    "train_y = iris[\"target\"]\n",
    "\n",
    "# Fit model\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10, random_state=42)\n",
    "softmax_reg.fit(train_x, train_y)\n",
    "\n",
    "# Generate values for petal length and petal width\n",
    "test_featvec_x0, test_featvec_x1 = jnp.meshgrid(\n",
    "    jnp.linspace(train_x[:, 0].min() - 0.5, train_x[:, 0].max() + 1, 500),\n",
    "    jnp.linspace(train_x[:, 1].min() - 0.5, train_x[:, 1].max() + 2, 500),\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(\n",
    "    train_x[train_y == 2, 0],\n",
    "    train_x[train_y == 2, 1],\n",
    "    c=\"g\",\n",
    "    marker=\"^\",\n",
    "    zorder=2,\n",
    "    label=\"Iris-Virginica\",\n",
    ")\n",
    "plt.scatter(\n",
    "    train_x[train_y == 1, 0],\n",
    "    train_x[train_y == 1, 1],\n",
    "    c=\"y\",\n",
    "    marker=\"o\",\n",
    "    zorder=2,\n",
    "    label=\"Iris-Versicolor\",\n",
    ")\n",
    "plt.scatter(\n",
    "    train_x[train_y == 0, 0],\n",
    "    train_x[train_y == 0, 1],\n",
    "    c=\"b\",\n",
    "    marker=\"s\",\n",
    "    zorder=2,\n",
    "    label=\"Iris-Setosa\",\n",
    ")\n",
    "\n",
    "\n",
    "# Combine new values of petal length, petal width\n",
    "test_x = jnp.c_[test_featvec_x0.ravel(), test_featvec_x1.ravel()]\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = softmax_reg.predict_proba(test_x)\n",
    "\n",
    "# Make predictions\n",
    "y_predict = softmax_reg.predict(test_x)\n",
    "\n",
    "# Choose probability for label Iris-Versicolor\n",
    "prob_iris_versicolor = y_pred_proba[:, 1].reshape(test_featvec_x0.shape)\n",
    "y_predict = y_predict.reshape(test_featvec_x0.shape)\n",
    "\n",
    "custom_cmap = ListedColormap([\"#9898ff\", \"#fafab0\", \"#a0faa0\"])\n",
    "plt.contourf(test_featvec_x0, test_featvec_x1, y_predict, cmap=custom_cmap)\n",
    "contour = plt.contour(test_featvec_x0, test_featvec_x1, prob_iris_versicolor, cmap=plt.cm.brg)\n",
    "clb = plt.clabel(contour, inline=1)\n",
    "\n",
    "plt.xlabel(\"Petal length(cm)\")\n",
    "plt.ylabel(\"Petal width(cm)\")\n",
    "plt.legend(loc=\"upper left\", framealpha=0.5, fontsize=11)\n",
    "savefig(\"iris-logreg-2d-3class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ade1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 2.13 - Use 2 features and all 3 classes\n",
    "\n",
    "latexify(width_scale_factor=1.55)\n",
    "\n",
    "# Intialize Plots and other options\n",
    "if is_latexify_enabled():\n",
    "    fig = plt.figure()\n",
    "    SCATTER_SIZE = 10\n",
    "    legend_fontsize = 5\n",
    "    line_width = 0.8\n",
    "else:\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    SCATTER_SIZE = 45\n",
    "    legend_fontsize = 11\n",
    "    line_width = 1.5\n",
    "\n",
    "# Get axis of current plot\n",
    "ax = plt.gca()\n",
    "\n",
    "# Declare cmaps for countors\n",
    "cmaps = [\"Blues\", \"Purples\", \"Greens\"]\n",
    "\n",
    "# Store probabilities for each class\n",
    "prob_iris_setosa = y_pred_proba[:, 0].reshape(test_featvec_x0.shape)\n",
    "prob_iris_versicolor = y_pred_proba[:, 1].reshape(test_featvec_x0.shape)\n",
    "prob_iris_virginica = y_pred_proba[:, 2].reshape(test_featvec_x0.shape)\n",
    "\n",
    "# Plot countours\n",
    "plt.contourf(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_setosa,\n",
    "    cmap=cmaps[0],\n",
    "    alpha=0.3,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_setosa,\n",
    "    cmap=cmaps[0],\n",
    "    alpha=0.4,\n",
    "    linewidths=line_width,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "plt.contourf(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_versicolor,\n",
    "    cmap=cmaps[1],\n",
    "    alpha=0.3,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_versicolor,\n",
    "    cmap=cmaps[1],\n",
    "    alpha=0.4,\n",
    "    linewidths=line_width,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "plt.contourf(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_virginica,\n",
    "    cmap=cmaps[2],\n",
    "    alpha=0.3,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    test_featvec_x0,\n",
    "    test_featvec_x1,\n",
    "    prob_iris_virginica,\n",
    "    cmap=cmaps[2],\n",
    "    alpha=0.4,\n",
    "    linewidths=line_width,\n",
    "    levels=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    ")\n",
    "\n",
    "# Plot training data\n",
    "plt.scatter(\n",
    "    train_x[train_y == 0, 0],\n",
    "    train_x[train_y == 0, 1],\n",
    "    s=SCATTER_SIZE,\n",
    "    c=\"blue\",\n",
    "    marker=\"s\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.01,\n",
    "    label=\"Setosa\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    train_x[train_y == 1, 0],\n",
    "    train_x[train_y == 1, 1],\n",
    "    s=SCATTER_SIZE,\n",
    "    c=\"purple\",\n",
    "    marker=\"o\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.01,\n",
    "    label=\"Versicolor\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    train_x[train_y == 2, 0],\n",
    "    train_x[train_y == 2, 1],\n",
    "    s=SCATTER_SIZE,\n",
    "    c=\"green\",\n",
    "    marker=\"^\",\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.01,\n",
    "    label=\"Virginica\",\n",
    ")\n",
    "\n",
    "# Labels,title and legends for scatter plot\n",
    "ax.set_xlabel(\"Petal length (cm)\")\n",
    "ax.set_ylabel(\"Petal width (cm)\")\n",
    "ax.set_title(\"Predictions\")\n",
    "ax.set_xlim([0.8, 7])\n",
    "ax.set_ylim([0, 3.5])\n",
    "ax.legend(loc=\"upper left\", fontsize=legend_fontsize, framealpha=0.5)\n",
    "\n",
    "savefig(\"iris_logreg_2d_latexified\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a78967",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_x[train_y == 2, 0], train_x[train_y == 2, 1], \"g^\", label=\"Iris-Virginica\")\n",
    "plt.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], \"yo\", label=\"Iris-Versicolor\")\n",
    "plt.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], \"bs\", label=\"Iris-Setosa\")\n",
    "\n",
    "plt.contourf(test_featvec_x0, test_featvec_x1, y_predict, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length(cm)\", fontsize=11)\n",
    "plt.ylabel(\"Petal width(cm)\", fontsize=11)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.axis([0.8, 7, 0, 3.5])\n",
    "savefig(\"iris-logreg-2d-3class-noprobs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc27a7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made 10 errors out of 50, on instances [ 4 15 21 32 35 36 40 41 42 48]\n",
      "Error rates on train 0.180 and test 0.200\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "# Fit model and evaluate on separate test set\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# We only take the first two features to make problem harder\n",
    "data_x = iris.data[:, :2]\n",
    "data_y = iris.target\n",
    "\n",
    "# Train Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Fit models\n",
    "logreg = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", C=1000)\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "# Make Predicitons\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "# Calculate Errors\n",
    "errs = y_pred != y_test\n",
    "count_errs = jnp.sum(errs)\n",
    "print(\"Made {} errors out of {}, on instances {}\".format(count_errs, len(y_pred), jnp.where(errs)[0]))\n",
    "\n",
    "# Zero One classification loss\n",
    "err_rate_test = zero_one_loss(y_test, y_pred)\n",
    "assert jnp.isclose(err_rate_test, count_errs / len(y_pred))\n",
    "err_rate_train = zero_one_loss(y_train, logreg.predict(x_train))\n",
    "print(\"Error rates on train {:0.3f} and test {:0.3f}\".format(err_rate_train, err_rate_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf2f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
